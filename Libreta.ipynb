{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13975917,"sourceType":"datasetVersion","datasetId":8910050}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":6512.386088,"end_time":"2025-12-13T20:48:25.548259","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-12-13T18:59:53.162171","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"0036c5f654cc4b1ea3da613395ee8ee8":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_fd9aec91d2c94a2d8f42cb0024be23d7","placeholder":"​","style":"IPY_MODEL_01f3b0be20d240939a9645a1bc8a23eb","tabbable":null,"tooltip":null,"value":"pytorch_model.bin: 100%"}},"01f3b0be20d240939a9645a1bc8a23eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"06fe60df31d8478d85c99fa85b0061d2":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_5de4bde39b524d80b796b62a9cbc8538","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_99e2422608474a8caa7c4a02cc153d46","tabbable":null,"tooltip":null,"value":1}},"0cd25582904547caa9695a594bd424c4":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_630b254778df4c2eae743cf1126c46df","placeholder":"​","style":"IPY_MODEL_257160d53f1443839f11a9aceaf2ded6","tabbable":null,"tooltip":null,"value":" 346M/346M [00:01&lt;00:00, 283MB/s]"}},"1b9a410418804c86b44c68a9a7cc04f6":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"257160d53f1443839f11a9aceaf2ded6":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"33c8d4fc6b5b48d0920378deba3e90ea":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"382ef69549ca455c8c56d192c4a6849c":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4bed2b7c48014ca9a0af936a0c4b6c45":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c84fbad94cc74f0995e9e24ddaaf40a5","IPY_MODEL_06fe60df31d8478d85c99fa85b0061d2","IPY_MODEL_c14471d73d8a4c04a4d1fe2e9ca4bd9b"],"layout":"IPY_MODEL_382ef69549ca455c8c56d192c4a6849c","tabbable":null,"tooltip":null}},"55a881471e344c3bba7848327114a7ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5a858f138f6b492cb671390998c3d291":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5de4bde39b524d80b796b62a9cbc8538":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"630b254778df4c2eae743cf1126c46df":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ea2dca9080d49e0b796f74ed0a675b7":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72850c96517d481b80bb82d447e57272":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_5a858f138f6b492cb671390998c3d291","max":346351599,"min":0,"orientation":"horizontal","style":"IPY_MODEL_55a881471e344c3bba7848327114a7ed","tabbable":null,"tooltip":null,"value":346351599}},"8743116027b847b8a2023c4e502480b3":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0036c5f654cc4b1ea3da613395ee8ee8","IPY_MODEL_72850c96517d481b80bb82d447e57272","IPY_MODEL_0cd25582904547caa9695a594bd424c4"],"layout":"IPY_MODEL_1b9a410418804c86b44c68a9a7cc04f6","tabbable":null,"tooltip":null}},"99e2422608474a8caa7c4a02cc153d46":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bf4f1154ddd0499f85162a2bf2dbfe48":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"c14471d73d8a4c04a4d1fe2e9ca4bd9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_33c8d4fc6b5b48d0920378deba3e90ea","placeholder":"​","style":"IPY_MODEL_bf4f1154ddd0499f85162a2bf2dbfe48","tabbable":null,"tooltip":null,"value":" 69.7k/? [00:00&lt;00:00, 7.31MB/s]"}},"c84fbad94cc74f0995e9e24ddaaf40a5":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_6ea2dca9080d49e0b796f74ed0a675b7","placeholder":"​","style":"IPY_MODEL_d3c93d6a102741628a995aa56ff9e245","tabbable":null,"tooltip":null,"value":"config.json: "}},"d3c93d6a102741628a995aa56ff9e245":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"fd9aec91d2c94a2d8f42cb0024be23d7":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Proyecto de Visión Artificial y Reconocimiento de Patrones\n\n## Reconocimiento Visual de Videojuegos\n* Pablo\n* Alberto Pérez Álvarez","metadata":{"papermill":{"duration":0.011616,"end_time":"2025-12-13T18:59:56.971555","exception":false,"start_time":"2025-12-13T18:59:56.959939","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## 1. Introducción\n\nEn esta práctica aplicaremos **Deep Learning** para clasificar imágenes satelitales del conjunto de datos **EuroSAT**. El objetivo es identificar el uso del suelo (bosques, zonas industriales, cultivos, etc.) utilizando tanto redes diseñadas desde cero como técnicas de Transfer Learning.\n\n### Instrucciones de Dataset (Kaggle)\nPara que esta libreta funcione, debes añadir el siguiente dataset a tu entorno (en principio, si clonais la libreta lo deberíais de tener por defecto):\n1.  En el menú derecho, pulsa **Add Input**.\n2.  Busca **\"EuroSAT Dataset\"** (del usuario *Gota Dahiya*).\n3.  Añádelo pulsando el botón `+`.\n\n### Configuración Inicial\nAsegúrate de tener activada la **GPU** (T4 x2 o P100) para acelerar el entrenamiento.","metadata":{"papermill":{"duration":0.011884,"end_time":"2025-12-13T18:59:56.994972","exception":false,"start_time":"2025-12-13T18:59:56.983088","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#!pip install \"protobuf==3.20.3\" > /dev/null 2>&1  #Si no, da un error de compatibilidad entre liberías.\nimport os\n\n# Configuración de entorno\nos.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models, callbacks\nfrom tensorflow.keras.applications import EfficientNetV2B0\nfrom transformers import TFViTModel\nfrom sklearn.utils import class_weight\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport pathlib\nimport PIL.Image\nimport time\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport gc ","metadata":{"execution":{"iopub.status.busy":"2025-12-30T20:59:20.474865Z","iopub.execute_input":"2025-12-30T20:59:20.475661Z","iopub.status.idle":"2025-12-30T20:59:20.482000Z","shell.execute_reply.started":"2025-12-30T20:59:20.475620Z","shell.execute_reply":"2025-12-30T20:59:20.481256Z"},"papermill":{"duration":31.298365,"end_time":"2025-12-13T19:00:28.304581","exception":false,"start_time":"2025-12-13T18:59:57.006216","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reproducibilidad\nSEED = 2025\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\n\nprint(f\"TensorFlow Version: {tf.__version__}\")\nprint(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")","metadata":{"execution":{"iopub.status.busy":"2025-12-30T20:47:12.909092Z","iopub.execute_input":"2025-12-30T20:47:12.909410Z","iopub.status.idle":"2025-12-30T20:47:12.934930Z","shell.execute_reply.started":"2025-12-30T20:47:12.909382Z","shell.execute_reply":"2025-12-30T20:47:12.934142Z"},"papermill":{"duration":0.117629,"end_time":"2025-12-13T19:00:28.434513","exception":false,"start_time":"2025-12-13T19:00:28.316884","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## 2. Carga y Gestión de Datos\n\nUtilizaremos `tf.keras.utils.image_dataset_from_directory`. Esta función crea un generador de datos que carga las imágenes del disco bajo demanda, evitando saturar la memoria RAM.","metadata":{"papermill":{"duration":0.011922,"end_time":"2025-12-13T19:00:28.458644","exception":false,"start_time":"2025-12-13T19:00:28.446722","status":"completed"},"tags":[]}},{"cell_type":"code","source":"KAGGLE_PATH = '/kaggle/input/videojuegos/images_dataset'\n# Ruta que tendria en local.\nLOCAL_PATH = './images_dataset' \n\n# Kaggle siempre define la variable de entorno 'KAGGLE_KERNEL_RUN_TYPE'\nif os.environ.get('KAGGLE_KERNEL_RUN_TYPE') is not None:\n    print(\"Entorno de Kaggle\")\n    data_dir = KAGGLE_PATH\nelse:\n    data_dir = LOCAL_PATH\n    print(\"Entorno Local:\",data_dir)\n    \ndata_dir = pathlib.Path(data_dir)\n\n# Verificación del contenido\nall_images = list(data_dir.glob('*/*.jpg'))\nimage_count = len(all_images)\nprint(f\"Total de imágenes encontradas: {image_count}\")\n\n# Verificación de dimensiones reales\nfirst_image = PIL.Image.open(all_images[0])\nprint(f\"Dimensiones reales de una imagen de muestra: {first_image.size}\")\nprint(f\"Formato de imagen: {first_image.format}\")\n\n# Parámetros Globales\n# Ajustamos las constantes al tamaño real detectado (debería ser 64x64)\nBATCH_SIZE = 64\nIMG_HEIGHT = 64\nIMG_WIDTH = 64\n\"\"\"BATCH_SIZE = 16\nIMG_HEIGHT = 480\nIMG_WIDTH = 854\"\"\"","metadata":{"execution":{"iopub.status.busy":"2025-12-30T20:59:22.142059Z","iopub.execute_input":"2025-12-30T20:59:22.142429Z","iopub.status.idle":"2025-12-30T20:59:22.168087Z","shell.execute_reply.started":"2025-12-30T20:59:22.142400Z","shell.execute_reply":"2025-12-30T20:59:22.167413Z"},"papermill":{"duration":0.734205,"end_time":"2025-12-13T19:00:29.204503","exception":false,"start_time":"2025-12-13T19:00:28.470298","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.1 Creación de Pipelines (`tf.data`) y División Train/Val/Test\n\nPara garantizar la robustez de los resultados, dividiremos los datos de la siguiente manera:\n* **70% Entrenamiento:** Para ajustar los pesos de los modelos.\n* **15% Validación:** Para ajustar hiperparámetros y *Early Stopping*.\n* **15% Test:** Conjunto para la evaluación final.","metadata":{"papermill":{"duration":0.012005,"end_time":"2025-12-13T19:00:29.229031","exception":false,"start_time":"2025-12-13T19:00:29.217026","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def do_pipeline(data_dir, img_height, img_width, batch_size, seed=123):\n    \"\"\"\n    Carga el dataset, lo redimensiona y lo divide en Train, Val y Test.\n    Retorna: (train_ds, val_ds, test_ds, class_names)\n    \"\"\"\n    print(f\"--- Cargando dataset con tamaño: {img_height}x{img_width} y Batch: {batch_size} ---\")\n    \n    full_ds = tf.keras.utils.image_dataset_from_directory(\n        data_dir,\n        seed=seed,\n        image_size=(img_height, img_width),\n        batch_size=batch_size,\n        shuffle=True,\n        label_mode='int'\n    )\n    \n    class_names = full_ds.class_names\n    print(f\"Clases encontradas: {class_names}\")\n    \n    n_batches = tf.data.experimental.cardinality(full_ds).numpy()\n    \n    train_size = int(0.7 * n_batches)\n    val_size = int(0.15 * n_batches)\n    test_size = n_batches - train_size - val_size\n    \n    print(f\"Total batches: {n_batches} -> Train: {train_size}, Val: {val_size}, Test: {test_size}\")\n    \n    train_ds = full_ds.take(train_size)\n    remaining_ds = full_ds.skip(train_size)\n    val_ds = remaining_ds.take(val_size)\n    test_ds = remaining_ds.skip(val_size)\n    \n    return train_ds, val_ds, test_ds, class_names, full_ds\n\ntrain_ds, val_ds, test_ds, class_names, full_ds = do_pipeline(data_dir,IMG_HEIGHT,IMG_WIDTH,BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2025-12-30T20:59:24.003249Z","iopub.execute_input":"2025-12-30T20:59:24.004030Z","iopub.status.idle":"2025-12-30T20:59:24.452019Z","shell.execute_reply.started":"2025-12-30T20:59:24.003991Z","shell.execute_reply":"2025-12-30T20:59:24.451254Z"},"papermill":{"duration":18.732832,"end_time":"2025-12-13T19:00:47.973817","exception":false,"start_time":"2025-12-13T19:00:29.240985","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Calculo de pesos para contrarrestar el desbalanceo de datos","metadata":{}},{"cell_type":"code","source":"def calculate_class_weights(train_ds):\n    \"\"\"\n    Extrae las etiquetas del dataset de entrenamiento y calcula los pesos\n    para equilibrar las clases durante el entrenamiento.\n    \"\"\"\n    print(\"Calculando pesos de las clases \")\n    \n    # Extraemos todas las etiquetas (y) del dataset\n    # Iteramos sobre el dataset y concatenamos solo las etiquetas (y)\n    train_labels = np.concatenate([y.numpy() for x, y in train_ds], axis=0)\n    \n    classes = np.unique(train_labels)\n    \n    weights = class_weight.compute_class_weight(\n        class_weight='balanced',\n        classes=classes,\n        y=train_labels\n    )\n    \n    # Convertimos a un diccionario {indice_clase: peso} que es lo que pide Keras\n    class_weight_dict = dict(zip(classes, weights))\n    \n    print(\"Pesos calculados:\")\n    for cls, weight in class_weight_dict.items():\n        print(f\"  Clase {cls}: {weight:.4f}\")\n        \n    return class_weight_dict\n\nclass_weights = calculate_class_weights(train_ds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T20:59:25.838370Z","iopub.execute_input":"2025-12-30T20:59:25.839159Z","iopub.status.idle":"2025-12-30T20:59:28.030797Z","shell.execute_reply.started":"2025-12-30T20:59:25.839128Z","shell.execute_reply":"2025-12-30T20:59:28.029958Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.2 Optimización (Performance)\nUsamos `.cache()` y `.prefetch()` para que la GPU no tenga que esperar a la carga de datos.","metadata":{"papermill":{"duration":0.012065,"end_time":"2025-12-13T19:00:47.998373","exception":false,"start_time":"2025-12-13T19:00:47.986308","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def do_performance(train_ds, val_ds, test_ds):\n    AUTOTUNE = tf.data.AUTOTUNE\n    \n    train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n    val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n    test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n    \n    return train_ds, val_ds, test_ds\n\ntrain_ds, val_ds, test_ds = do_performance(train_ds, val_ds, test_ds)","metadata":{"execution":{"iopub.status.busy":"2025-12-30T20:59:28.032175Z","iopub.execute_input":"2025-12-30T20:59:28.032543Z","iopub.status.idle":"2025-12-30T20:59:28.049616Z","shell.execute_reply.started":"2025-12-30T20:59:28.032517Z","shell.execute_reply":"2025-12-30T20:59:28.048905Z"},"papermill":{"duration":0.030228,"end_time":"2025-12-13T19:00:48.040317","exception":false,"start_time":"2025-12-13T19:00:48.010089","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.3 Análisis Exploratorio (EDA)\nEn esta práctica no nos vamos a centar en el EDA y preprocesamiento. Solo se piden (obligatoriamente) las siguientes tareas:\n\n**Tarea:**\n1.  Visualiza una cuadrícula de imágenes del conjunto de entrenamiento con sus etiquetas (preferiblemente mostrando alguna de cada clase).\n2.  Analiza si el dataset está balanceado mostrando un gráfico de barras con la cantidad de imágenes por clase.","metadata":{"papermill":{"duration":0.011958,"end_time":"2025-12-13T19:00:48.064445","exception":false,"start_time":"2025-12-13T19:00:48.052487","status":"completed"},"tags":[]}},{"cell_type":"code","source":"plt.figure(figsize=(15, 10))\n\n# 1. Usamos 'unbatch()' para sacar las imágenes de los paquetes\n# 2. Usamos 'take(25)' para coger exactamente las que necesitamos\n# 3. Usamos enumerate para saber en qué posición (i) del subplot estamos\nfor i, (image, label) in enumerate(train_ds.unbatch().take(25)):\n    \n    ax = plt.subplot(5, 5, i + 1)\n    \n    # Ya no necesitamos [i] porque 'image' es una sola foto, no un lote\n    plt.imshow(image.numpy().astype(\"uint8\"))\n    \n    # Manejo del label\n    # Nota: Dependiendo de tu dataset, label podría ser un escalar o un array de 1 elemento\n    label_index = int(label) \n    plt.title(class_names[label_index])\n    plt.axis(\"off\")\n\nplt.suptitle(\"Imagenes de ejemplo del dataset\", fontsize=16)\nplt.tight_layout()\nplt.show()\n\n\nclass_counts = {name: 0 for name in class_names}\nfor _, labels in full_ds:\n    for label in labels:\n        class_name = class_names[int(label)]\n        class_counts[class_name] += 1\n\n# Lo hacemos con un dataframe, pues es mas facil hacer el plot.\ndf_counts = pd.DataFrame(list(class_counts.items()), columns=['Class', 'Count'])\n\nplt.figure(figsize=(12, 6))\nbplot = sns.barplot(x='Count', y='Class', data=df_counts, palette='viridis', hue='Class')\n# tenemos que quitar la leyenda manualmente porque da error con legend=False\nif bplot.get_legend() is not None:\n    bplot.get_legend().remove()\n\nplt.title('Distribucion de imagenes en el dataset', fontsize=16)\nplt.xlabel('Numero de imagenes', fontsize=12)\nplt.ylabel('Clase', fontsize=12)\nplt.grid(axis='x', linestyle='--', alpha=0.7)\n\n# añadimos el numero al final de la barra para verlo mejor\nfor index, value in enumerate(df_counts['Count']):\n    plt.text(value + 50, index, str(value), va='center')\n\nplt.tight_layout()\nplt.show()\n\nmean_count = df_counts['Count'].mean()\nstd_count = df_counts['Count'].std()\nprint(f\"Media de imagenes: {mean_count:.2f}\")\nprint(f\"Desviacion estandar: {std_count:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2025-12-30T20:59:30.093574Z","iopub.execute_input":"2025-12-30T20:59:30.093956Z","iopub.status.idle":"2025-12-30T20:59:36.423338Z","shell.execute_reply.started":"2025-12-30T20:59:30.093919Z","shell.execute_reply":"2025-12-30T20:59:36.421912Z"},"papermill":{"duration":49.489405,"end_time":"2025-12-13T19:01:37.565803","exception":false,"start_time":"2025-12-13T19:00:48.076398","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Como podemos observar en el grafico de barras, y confirmado por la media y la desviación, existe un claro desbalance entre los datos, aunque no hay ninguna clase con muchos más datos. Sin embargo la clase Pasture cuenta con bastantes menos datos que las máximas.Esto nos condicionará a la hora de entrenar pues habrá que tener en cuenta aquellas clases que no cuenten con tantos ejemplos.","metadata":{"papermill":{"duration":0.025218,"end_time":"2025-12-13T19:01:37.616648","exception":false,"start_time":"2025-12-13T19:01:37.591430","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"---\n## 3. Baseline: Perceptrón Multicapa (MLP)\n\nAntes de usar redes convolucionales, diseñaremos una red densa clásica (`Sequential`) como baseline para comprobar las limitaciones de tratar imágenes como datos tabulares planos.\n\n**Requisitos:**\n1.  La red debe comenzar normalizando los valores de entrada ($0-1$) y aplanando la imagen para convertir la matriz 3D en un vector 1D.\n2.  Eres libre de elegir la profundidad y anchura de la red. Ten en cuenta que la entrada tiene más de $12.000$ dimensiones ($64 \\times 64 \\times 3$).\n3.  Configura correctamente la última capa para un problema de clasificación de 10 clases.\n4.  Compila el modelo utilizando el optimizador `adam` y la función de pérdida `sparse_categorical_crossentropy`.\n5.  Se recomienda guardar el **tiempo de entrenamiento**, en este y todos los modelos, para posibles comparaciones posteriores.\n6.  Guarda el mejor modelo generado como `best_mlp.keras`.\n\n**Evaluación:**\n1.  Utiliza `accuracy` como métrica de monitorización. Pensad por qué podemos usar esta métrica.\n2.  Genera y analiza las gráficas de `loss` y `accuracy` (Entrenamiento vs. Validación).\n3.  **Pregunta:** ¿Por qué, teóricamente, un MLP no es la arquitectura ideal para este problema comparado con una CNN?","metadata":{"papermill":{"duration":0.023399,"end_time":"2025-12-13T19:01:37.663165","exception":false,"start_time":"2025-12-13T19:01:37.639766","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Funcion que usamos para imprimir las graficas de los distintos modelos\ndef plot_history(history, model_name, stop_epoch=None):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    f1 = history.history['f1_score']\n    val_f1 = history.history['val_f1_score']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    \n    epochs_range = range(len(acc)) \n    \n    plt.figure(figsize=(12, 10))\n    \n    plt.subplot(2, 2, 1)\n    plt.plot(epochs_range, acc, label='Training accuracy')\n    plt.plot(epochs_range, val_acc, label='Validation accuracy')\n    if stop_epoch is not None:\n        plt.axvline(x=stop_epoch, color='r', linestyle='--', label=f'Early Stopping (Ep {stop_epoch+1})')\n    plt.legend(loc='lower right')\n    plt.title(f'{model_name}: Accuracy')\n    plt.grid(True)\n\n    plt.subplot(2, 2, 3)\n    plt.plot(epochs_range, f1, label='Training f1')\n    plt.plot(epochs_range, val_f1, label='Validation f1')\n    if stop_epoch is not None:\n        plt.axvline(x=stop_epoch, color='r', linestyle='--', label=f'Early Stopping (Ep {stop_epoch+1})')\n    plt.legend(loc='lower right')\n    plt.title(f'{model_name}: F1-score')\n    plt.grid(True)\n    \n    plt.subplot(2, 2, 2)\n    plt.plot(epochs_range, loss, label='Pérdida de training')\n    plt.plot(epochs_range, val_loss, label='Perdida de validation')\n    plt.legend(loc='upper right')\n    plt.title(f'{model_name}: Perdida')\n    plt.grid(True)\n    \n    plt.tight_layout() # Para que se muestren bien con márgenes\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2025-12-30T20:59:38.585418Z","iopub.execute_input":"2025-12-30T20:59:38.585698Z","iopub.status.idle":"2025-12-30T20:59:38.593978Z","shell.execute_reply.started":"2025-12-30T20:59:38.585673Z","shell.execute_reply":"2025-12-30T20:59:38.593218Z"},"papermill":{"duration":0.039028,"end_time":"2025-12-13T19:01:37.725478","exception":false,"start_time":"2025-12-13T19:01:37.686450","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SparseF1Score(tf.keras.metrics.F1Score):\n    def update_state(self, y_true, y_pred, sample_weight=None):\n       \n        num_classes = tf.shape(y_pred)[-1]  # numero de clases de la predicción\n        y_true = tf.reshape(y_true, [-1])\n        y_true_one_hot = tf.one_hot(tf.cast(y_true, tf.int32), num_classes) # Convertimos los enteros (sparse) a One-Hot\n        \n        super().update_state(y_true_one_hot, y_pred, sample_weight) # Llamamos a la función original con los datos corregidos","metadata":{"execution":{"iopub.status.busy":"2025-12-30T20:59:42.886591Z","iopub.execute_input":"2025-12-30T20:59:42.886881Z","iopub.status.idle":"2025-12-30T20:59:42.892095Z","shell.execute_reply.started":"2025-12-30T20:59:42.886854Z","shell.execute_reply":"2025-12-30T20:59:42.891326Z"},"papermill":{"duration":0.039003,"end_time":"2025-12-13T19:01:37.798137","exception":false,"start_time":"2025-12-13T19:01:37.759134","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_mlp_model():\n    model = models.Sequential([\n        layers.InputLayer(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)), # El input es alto, ancho y n canales de RGB\n        \n        # Normalización de las capas (0-255 -> 0-1)\n        layers.Rescaling(1./255),\n        # 64 * 64 * 3 = 12,288 dimensiones, pero aplanando lo dejamos como un vector\n        layers.Flatten(),\n        \n        # Las capas densas estaran conectadas unas entre otras\n        layers.Dense(512, activation='relu'), # rectified linear unit\n        layers.Dense(256, activation='relu'), \n        layers.Dense(128, activation='relu'),\n        \n        # Hay 10 posibles outputs\n        layers.Dense(10, activation='softmax') # devuelve probabilidades para cada clase\n    ], name=\"MLP_Baseline\")\n    \n    return model\n\nmlp_model_ES = build_mlp_model()\nmlp_model_ES.summary()\n\nmlp_model_ES.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy', SparseF1Score(average='weighted', name='f1_score')]\n\n)\n\ncallbacks_list_mlp = [\n    callbacks.EarlyStopping( \n        monitor='val_loss', \n        patience=10, \n        restore_best_weights=True,\n        verbose=1\n    ),\n    callbacks.ModelCheckpoint(\n        filepath='best_mlp.keras',\n        monitor='val_accuracy',\n        save_best_only=True,\n        verbose=1\n    )\n]\n\nprint(\"\\nMLP con Early Stopping\")\nstart_time = time.time()\n\nhistory_mlp_ES = mlp_model_ES.fit(\n    train_ds,\n    epochs=50, # lo ponemos alto y lo parara el early stop.\n    validation_data=val_ds,\n    callbacks=callbacks_list_mlp\n)\n\nend_time = time.time()\nmlp_ES_training_time = end_time - start_time\nprint(f\"\\Tiempo entrenamiento MLP: {mlp_ES_training_time:.2f} s\")\n\nepoch_parada_MLP = len(history_mlp_ES.history['loss']) - 1","metadata":{"execution":{"iopub.status.busy":"2025-12-30T20:27:19.578373Z","iopub.execute_input":"2025-12-30T20:27:19.578712Z","iopub.status.idle":"2025-12-30T20:27:28.942202Z","shell.execute_reply.started":"2025-12-30T20:27:19.578680Z","shell.execute_reply":"2025-12-30T20:27:28.941467Z"},"papermill":{"duration":62.839132,"end_time":"2025-12-13T19:02:40.661502","exception":false,"start_time":"2025-12-13T19:01:37.822370","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Sin Early Stopping para comparar entre modelos\nOriginalmente únicamente lo ejecutábamos con Early Stopping, pero con el fin de visualizar mejor las gráficas y una mejor comparación con otros modelos (a igualdad de épocas), hemos decidido volver a entrenarlo sin Early Stopping.","metadata":{"papermill":{"duration":0.061624,"end_time":"2025-12-13T19:02:40.785614","exception":false,"start_time":"2025-12-13T19:02:40.723990","status":"completed"},"tags":[]}},{"cell_type":"code","source":"mlp_model = build_mlp_model()\n#mlp_model.summary()\n\nmlp_model.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy', SparseF1Score(average='weighted', name='f1_score')]\n\n)\n\ncallbacks_list_mlp_no_ES = [\n    callbacks.ModelCheckpoint(\n        filepath='best_mlp.keras',\n        monitor='val_accuracy',\n        save_best_only=True,\n        verbose=1\n    )\n]\n\nprint(\"\\nMLP\")\nstart_time = time.time()\n\nhistory_mlp = mlp_model.fit(\n    train_ds,\n    epochs=50, \n    validation_data=val_ds,\n    callbacks=callbacks_list_mlp_no_ES\n)\n\nend_time = time.time()\nmlp_training_time = end_time - start_time\nprint(f\"\\Tiempo entrenamiento MLP: {mlp_training_time:.2f} s\")","metadata":{"execution":{"iopub.status.busy":"2025-12-30T20:27:28.943299Z","iopub.execute_input":"2025-12-30T20:27:28.943781Z","iopub.status.idle":"2025-12-30T20:27:41.380619Z","shell.execute_reply.started":"2025-12-30T20:27:28.943756Z","shell.execute_reply":"2025-12-30T20:27:41.379986Z"},"papermill":{"duration":78.1895,"end_time":"2025-12-13T19:03:59.038013","exception":false,"start_time":"2025-12-13T19:02:40.848513","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Gráficas de accuracy y perdida","metadata":{"papermill":{"duration":0.117634,"end_time":"2025-12-13T19:03:59.272445","exception":false,"start_time":"2025-12-13T19:03:59.154811","status":"completed"},"tags":[]}},{"cell_type":"code","source":"plot_history(history_mlp, \"MLP Baseline\", epoch_parada_MLP)","metadata":{"execution":{"iopub.status.busy":"2025-12-30T20:27:41.381701Z","iopub.execute_input":"2025-12-30T20:27:41.382252Z","iopub.status.idle":"2025-12-30T20:27:41.832287Z","shell.execute_reply.started":"2025-12-30T20:27:41.382213Z","shell.execute_reply":"2025-12-30T20:27:41.831505Z"},"papermill":{"duration":0.644688,"end_time":"2025-12-13T19:04:00.037204","exception":false,"start_time":"2025-12-13T19:03:59.392516","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Tabla comparativa","metadata":{"papermill":{"duration":0.118173,"end_time":"2025-12-13T19:04:00.274201","exception":false,"start_time":"2025-12-13T19:04:00.156028","status":"completed"},"tags":[]}},{"cell_type":"code","source":"mlp_val_acc = max(history_mlp.history['val_accuracy'])\nmlp_val_f1 = max(history_mlp.history['val_f1_score'])\nmlp_time = mlp_training_time\n\nmlpES_val_acc = max(history_mlp_ES.history['val_accuracy'])\nmlpES_val_f1 = max(history_mlp_ES.history['val_f1_score'])\nmlpES_time = mlp_ES_training_time\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"      COMPARATIVA: MLP con vs sin Early Stopping\")\nprint(\"=\"*40)\nprint(f\"{'Métrica':<20} | {'MLP':<15} | {'MLP con ES':<15}\")\nprint(\"-\" * 56)\nprint(f\"{'Tiempo (s)':<20} | {mlp_time:.1f}            | {mlpES_time:.1f}\")\nprint(f\"{'Mejor Accuracy':<20} | {mlp_val_acc:.4f}          | {mlpES_val_acc:.4f}\")\nprint(f\"{'Mejor F1-Score':<20} | {mlp_val_f1:.4f}          | {mlpES_val_f1:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2025-12-30T20:27:41.833196Z","iopub.execute_input":"2025-12-30T20:27:41.833444Z","iopub.status.idle":"2025-12-30T20:27:41.839437Z","shell.execute_reply.started":"2025-12-30T20:27:41.833419Z","shell.execute_reply":"2025-12-30T20:27:41.838749Z"},"papermill":{"duration":0.12328,"end_time":"2025-12-13T19:04:00.514720","exception":false,"start_time":"2025-12-13T19:04:00.391440","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Evaluación\n1. Como el desbalance de datos no es muy grave (ninguna clase tiene la gran mayoría de datos), el uso de accuracy no debería ser demasiado perjudicial. Además se puede ver como la gráfica del f1 y la de accuracy son prácticamente iguales.\n\n\n2. Podemos ver cómo al principio el modelo generaliza muy bien, llegando a superar el accuracy de validation al de training. Sin embargo, a partir de la época 20 (y más aún en la 25) se aprecia un bajón en el accuracy de validation, posiblemente provocado porque el modelo está sobreajustando y aprendiéndose de memoria el dataset. Esta hipótesis se hace más fuerte al comprobar que la pérdida para validation también sufre de picos sobre la época 20 y se mantiene elevada hasta el final.\n\n\n3. Al aplanar la imágen perdemos información sobre el posicionamiento de cada píxel, que pasan de estar en una matriz  (con vecinos en dos dimensiones), a un vector (solo vecinos en dos lados, una dimensión). Esto provoca que el MLP no pueda aprender formas o texturas. Una CNN puede conservar la matriz 2D, por lo que no cuenta con esta limitación.\n\n    Además, un MLP no tiene resistencia al cambio de posición de objetos en las imágenes, por lo que si aprende lo que es un río que aparece en el lado superior de la imagen, tendrá que volver a aprenderlo para otra imágen en el que salga en la parte inferior. Nuevamente, una CNN no comparte esta limitación, pues tiene la capacidad de reconocer patrones a lo largo de la imagen, no únicamente en una posición.","metadata":{"papermill":{"duration":0.116887,"end_time":"2025-12-13T19:04:00.748395","exception":false,"start_time":"2025-12-13T19:04:00.631508","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"---\n## 4. Arquitectura Propia: Red Neuronal Convolucional (CNN)\n\nAhora diseñarás una arquitectura adaptada específicamente para el procesamiento de imágenes. Tu objetivo es **superar el rendimiento del Baseline (MLP)** aprovechando la capacidad de las CNN para aprender patrones espaciales y texturas.\n\nEres libre de definir la profundidad de la red y el número de filtros, pero debes cumplir los siguientes **requisitos**:\n1.  **Data Augmentation:** Incluye capas de preprocesamiento al inicio (ej. `RandomFlip`, `RandomRotation`) para mejorar la generalización.\n2.  **Estructura:** Se recomienda usar bloques repetitivos de `Conv2D` $\\rightarrow$ `MaxPooling2D`.\n    * *Advertencia:* Las imágenes son de $64 \\times 64$. Si abusas de las capas de *pooling*, reducirás la imagen a $1 \\times 1$ antes de tiempo, perdiendo información.\n3.  **Regularización:** Es obligatorio implementar `Dropout` y/o `BatchNormalization` para reducir el sobreajuste. Experimenta con su posición y valores.\n4.  **Callbacks:** Configura `EarlyStopping` y `ModelCheckpoint`.\n\n**Tarea:**\n1.  Diseña, compila y entrena tu modelo.\n2.  Compara las curvas de aprendizaje con las del MLP. ¿Converge más rápido? ¿Sufre menos sobreajuste?\n3.  Guarda el mejor modelo generado como `best_cnn.keras`.\n4.  Compara el ratio entre tamaño del modelo y accuracy del modelo generado (CNN) con el MLP anterior.\n5.  Se valorará el estudio de la influencia del aumento de datos y del uso de `Dropout` y/o `BatchNormalization`.","metadata":{"papermill":{"duration":0.115232,"end_time":"2025-12-13T19:04:00.979967","exception":false,"start_time":"2025-12-13T19:04:00.864735","status":"completed"},"tags":[]}},{"cell_type":"code","source":"BATCH_SIZE = 64\nIMG_HEIGHT = 224\nIMG_WIDTH = 224\n\ntrain_ds, val_ds, test_ds, class_names, full_ds = do_pipeline(data_dir,IMG_HEIGHT,IMG_WIDTH,BATCH_SIZE)\ntrain_ds, val_ds, test_ds = do_performance(train_ds, val_ds, test_ds)","metadata":{"execution":{"iopub.status.busy":"2025-12-30T20:27:41.840272Z","iopub.execute_input":"2025-12-30T20:27:41.840536Z","iopub.status.idle":"2025-12-30T20:27:42.271762Z","shell.execute_reply.started":"2025-12-30T20:27:41.840503Z","shell.execute_reply":"2025-12-30T20:27:42.271030Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_cnn_model():\n    # evitamos memorizar el dataset con data augmentation \n    # Giramos y hacemos zoom aleatorio a las imagenes mientras entrena\n    data_augmentation = models.Sequential([\n        layers.RandomFlip(\"horizontal_and_vertical\"),\n        layers.RandomRotation(0.2),\n        layers.RandomZoom(0.1),\n    ], name=\"data_augmentation\")\n    \n    inputs = layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3)) # 64x64 pixeles, 3 canales RGB\n    \n    x = data_augmentation(inputs)\n\n    # 0-255 a 0-1.\n    x = layers.Rescaling(1./255)(x)\n\n    # Convoluciones\n    \n    # Buscamos detalles simples como bordes o esquinas.\n    x = layers.Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x) # Esto ayuda a estabilizar el aprendizaje\n    x = layers.MaxPooling2D((2, 2))(x) # Reducimos la imagen a la mitad, 32x32\n\n    # Buscamos texturas o formas más definidas.\n    x = layers.Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D((2, 2))(x) # Bajamos a 16x16\n\n    x = layers.Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D((2, 2))(x) # Bajamos a 8x8\n\n    x = layers.Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D((2, 2))(x) # Bajamos a 4x4\n\n    # Los datos a un vector \n    x = layers.Flatten()(x)\n    x = layers.Dense(512, activation='relu')(x)\n    \n    # Hacemos dropout para apagar la mitad de las neuronas al azar y evitar overfitting\n    x = layers.Dropout(0.5)(x) \n    \n    # 10 outputs\n    outputs = layers.Dense(10, activation='softmax')(x)\n\n    model = keras.Model(inputs=inputs, outputs=outputs, name=\"Custom_CNN\")\n    return model\n\ncnn_model_ES = build_cnn_model()\ncnn_model_ES.summary()\n\ncnn_model_ES.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy', SparseF1Score(average='weighted', name='f1_score')]\n)\n\ncallbacks_list_cnn = [\n    callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=10, \n        restore_best_weights=True,\n        verbose=1\n    ),\n    callbacks.ModelCheckpoint(\n        filepath='best_cnn.keras',\n        monitor='val_accuracy',\n        save_best_only=True,\n        verbose=0 \n    )\n]\n\nprint(\"\\nCNN\")\nstart_time_cnn = time.time()\n\nhistory_cnn_ES = cnn_model_ES.fit(\n    train_ds,\n    epochs=50, # no deberia llegar a hacer los 50 por el early stop\n    validation_data=val_ds,\n    callbacks=callbacks_list_cnn,\n    verbose=1\n)\n\nend_time_cnn = time.time()\ncnn_ES_training_time = end_time_cnn - start_time_cnn\nprint(f\"\\nTiempo de entrenamiento CNN: {cnn_ES_training_time:.2f} s\")\nepoch_parada_CNN = len(history_cnn_ES.history['loss']) - 1","metadata":{"execution":{"iopub.status.busy":"2025-12-30T20:27:42.272757Z","iopub.execute_input":"2025-12-30T20:27:42.273102Z","iopub.status.idle":"2025-12-30T20:30:30.905289Z","shell.execute_reply.started":"2025-12-30T20:27:42.273078Z","shell.execute_reply":"2025-12-30T20:30:30.904522Z"},"papermill":{"duration":43.609304,"end_time":"2025-12-13T19:04:44.704456","exception":false,"start_time":"2025-12-13T19:04:01.095152","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Sin Early Stopping","metadata":{"papermill":{"duration":0.144679,"end_time":"2025-12-13T19:04:44.994652","exception":false,"start_time":"2025-12-13T19:04:44.849973","status":"completed"},"tags":[]}},{"cell_type":"code","source":"callbacks_list_cnn_no_ES = [\n    callbacks.ModelCheckpoint(\n        filepath='best_cnn.keras',\n        monitor='val_accuracy',\n        save_best_only=True,\n        verbose=0 \n    )\n]\n\ncnn_model = build_cnn_model()\ncnn_model.summary()\n\ncnn_model.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy', SparseF1Score(average='weighted', name='f1_score')]\n)\n\nprint(\"\\nCNN sin Early Stopping\")\nstart_time_cnn = time.time()\n\nhistory_cnn = cnn_model.fit(\n    train_ds,\n    epochs=50, \n    validation_data=val_ds,\n    callbacks=callbacks_list_cnn_no_ES,\n    verbose=1\n)\n\nend_time_cnn = time.time()\ncnn_training_time = end_time_cnn - start_time_cnn\nprint(f\"\\nTiempo de entrenamiento CNN sin Early Stop: {cnn_training_time:.2f} s\")\n","metadata":{"execution":{"iopub.status.busy":"2025-12-30T20:30:30.906351Z","iopub.execute_input":"2025-12-30T20:30:30.906669Z","iopub.status.idle":"2025-12-30T20:34:08.246343Z","shell.execute_reply.started":"2025-12-30T20:30:30.906645Z","shell.execute_reply":"2025-12-30T20:34:08.245671Z"},"papermill":{"duration":237.862903,"end_time":"2025-12-13T19:08:43.014725","exception":false,"start_time":"2025-12-13T19:04:45.151822","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# COMPARACION DE EFICIENCIA ENTRE CNN Y MLP\n# Intentamos recuperar los datos del MLP anterior para comparar.\n# Si no existen (por no haber corrido esa celda), ponemos ceros para que no falle el código.\ntry:\n    mlp_val_acc = max(history_mlp.history['val_accuracy'])\n    mlp_val_f1 = max(history_mlp.history['val_f1_score'])\n    mlp_time = mlp_model.count_params()\nexcept NameError:\n    mlp_val_acc = 0.0 \n    mlp_val_f1 = 0.0\n    mlp_time = 0    \n\ncnn_val_acc = max(history_cnn.history['val_accuracy'])\ncnn_val_f1 = max(history_cnn.history['val_f1_score'])\ncnn_params = cnn_model.count_params()\n\nprint(\"\\n\" + \"=\"*40)\nprint(\"      COMPARATIVA: MLP vs CNN\")\nprint(\"=\"*40)\nprint(f\"{'Métrica':<20} | {'MLP':<15} | {'CNN Propia':<15}\")\nprint(\"-\" * 56)\nprint(f\"{'Nº Parámetros':<20} | {mlp_time:<15,} | {cnn_params:<15,}\")\nprint(f\"{'Mejor Accuracy':<20} | {mlp_val_acc:.4f}          | {cnn_val_acc:.4f}\")\nprint(f\"{'Mejor F1-Score':<20} | {mlp_val_f1:.4f}          | {cnn_val_f1:.4f}\")\n\n\nif cnn_params > 0 and mlp_time > 0:\n    ratio_mlp = mlp_time / mlp_val_acc\n    ratio_cnn = cnn_params / cnn_val_acc\n    print(\"-\" * 56)\n    print(f\"{'Eficiencia':<20} | {int(ratio_mlp):<15,} | {int(ratio_cnn):<15,} (menos mejor)\")","metadata":{"execution":{"iopub.status.busy":"2025-12-30T20:34:08.247281Z","iopub.execute_input":"2025-12-30T20:34:08.247565Z","iopub.status.idle":"2025-12-30T20:34:08.254899Z","shell.execute_reply.started":"2025-12-30T20:34:08.247541Z","shell.execute_reply":"2025-12-30T20:34:08.254224Z"},"papermill":{"duration":0.309056,"end_time":"2025-12-13T19:08:43.630793","exception":false,"start_time":"2025-12-13T19:08:43.321737","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Comparación entre CNN y MLP","metadata":{"papermill":{"duration":0.29916,"end_time":"2025-12-13T19:08:44.225952","exception":false,"start_time":"2025-12-13T19:08:43.926792","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Como vemos en la tabla, la CNN cuenta con muchos menos parámetros y un accuracy bastante superior. Esto nos lleva a una mucha mejor eficiencia para el CNN.","metadata":{"papermill":{"duration":0.296008,"end_time":"2025-12-13T19:08:44.897226","exception":false,"start_time":"2025-12-13T19:08:44.601218","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### Gráficas de accuracy y perdida","metadata":{"papermill":{"duration":0.29543,"end_time":"2025-12-13T19:08:45.487932","exception":false,"start_time":"2025-12-13T19:08:45.192502","status":"completed"},"tags":[]}},{"cell_type":"code","source":"plot_history(history_cnn, \"Custom CNN\", epoch_parada_CNN)","metadata":{"execution":{"iopub.status.busy":"2025-12-30T20:34:08.255666Z","iopub.execute_input":"2025-12-30T20:34:08.255916Z","iopub.status.idle":"2025-12-30T20:34:08.693085Z","shell.execute_reply.started":"2025-12-30T20:34:08.255883Z","shell.execute_reply":"2025-12-30T20:34:08.692495Z"},"papermill":{"duration":0.803567,"end_time":"2025-12-13T19:08:46.597742","exception":false,"start_time":"2025-12-13T19:08:45.794175","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Evaluación\nLas gráficas demuestran que nuestra CNN sigue haciendo un poco de overfitting, pues la validación sigue estando por debajo del training. Sin embargo, esta vez la diferencia entre el validation y el training es constante a lo largo de las épocas. También podemos observar que el validation tiene muchos picos. Pensamos que esto ocurre por existir lotes de imágenes más \"confusas\" y otras más fáciles para el modelo. En el train esto no afecta tanto pues las imágenes se modifican con el data augmentation, pero  el validation al usar imágenes \"limpias\" es más sensible a este efecto. \n\nEn adición, comparando con el MLP anterior vemos como la CNN ya obtiene mejores resultados en sus primeras épocas, por lo que aprende más características más útiles y más rápido. Además la CNN tiene un sobreajuste que no aumenta con el tiempo, como sí ocurría con el MLP.","metadata":{"papermill":{"duration":0.297857,"end_time":"2025-12-13T19:08:47.276829","exception":false,"start_time":"2025-12-13T19:08:46.978972","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Estudio de Data Augmentation, Normalizacion y Dropout","metadata":{"papermill":{"duration":0.325305,"end_time":"2025-12-13T19:08:47.919496","exception":false,"start_time":"2025-12-13T19:08:47.594191","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### Sin Data Augmentation","metadata":{"papermill":{"duration":0.298288,"end_time":"2025-12-13T19:08:48.512919","exception":false,"start_time":"2025-12-13T19:08:48.214631","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def build_cnn_model_without_DA():\n    # evitamos memorizar el dataset con data augmentation \n    # Giramos y hacemos zoom aleatorio a las imagenes mientras entrena\n    \n    inputs = layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3)) # 64x64 pixeles, 3 canales RGB\n    \n    x = inputs\n\n    # 0-255 a 0-1.\n    x = layers.Rescaling(1./255)(x)\n\n    # Convoluciones\n    \n    # Buscamos detalles simples como bordes o esquinas.\n    x = layers.Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x) # Esto ayuda a estabilizar el aprendizaje\n    x = layers.MaxPooling2D((2, 2))(x) # Reducimos la imagen a la mitad, 32x32\n\n    # Buscamos texturas o formas más definidas.\n    x = layers.Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D((2, 2))(x) # Bajamos a 16x16\n\n    x = layers.Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D((2, 2))(x) # Bajamos a 8x8\n\n    x = layers.Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D((2, 2))(x) # Bajamos a 4x4\n\n    # Los datos a un vector \n    x = layers.Flatten()(x)\n    x = layers.Dense(512, activation='relu')(x)\n    \n    # Hacemos dropout para apagar la mitad de las neuronas al azar y evitar overfitting\n    x = layers.Dropout(0.5)(x) \n    \n    # 10 outputs\n    outputs = layers.Dense(10, activation='softmax')(x)\n\n    model = keras.Model(inputs=inputs, outputs=outputs, name=\"Custom_CNN\")\n    return model\n\ncnn_model = build_cnn_model_without_DA()\ncnn_model.summary()\n\ncnn_model.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy', SparseF1Score(average='weighted', name='f1_score')]\n)\n\ncallbacks_list_cnn = [\n    callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=10, \n        restore_best_weights=True,\n        verbose=1\n    )\n]\n\nprint(\"\\nCNN sin DA\")\nstart_time_cnn_w_DA = time.time()\n\nhistory_cnn_without_DA = cnn_model.fit(\n    train_ds,\n    epochs=50, # no deberia llegar a hacer los 50 por el early stop\n    validation_data=val_ds,\n    callbacks=callbacks_list_cnn,\n    verbose=1\n)\n\nend_time_cnn = time.time()\ncnn_training_time_w_DA = end_time_cnn - start_time_cnn_w_DA\nprint(f\"\\nTiempo de entrenamiento CNN: {cnn_training_time_w_DA:.2f} s\")","metadata":{"execution":{"iopub.status.busy":"2025-12-30T20:34:08.693933Z","iopub.execute_input":"2025-12-30T20:34:08.694224Z","iopub.status.idle":"2025-12-30T20:34:55.329094Z","shell.execute_reply.started":"2025-12-30T20:34:08.694174Z","shell.execute_reply":"2025-12-30T20:34:55.328469Z"},"papermill":{"duration":89.827803,"end_time":"2025-12-13T19:10:18.732076","exception":false,"start_time":"2025-12-13T19:08:48.904273","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Sin Normalizacion","metadata":{"papermill":{"duration":0.455623,"end_time":"2025-12-13T19:10:19.555129","exception":false,"start_time":"2025-12-13T19:10:19.099506","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def build_cnn_model_without_N():\n    # evitamos memorizar el dataset con data augmentation \n    # Giramos y hacemos zoom aleatorio a las imagenes mientras entrena\n    data_augmentation = models.Sequential([\n        layers.RandomFlip(\"horizontal_and_vertical\"),\n        layers.RandomRotation(0.2),\n        layers.RandomZoom(0.1),\n    ], name=\"data_augmentation\")\n    \n    inputs = layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3)) # 64x64 pixeles, 3 canales RGB\n    \n    x = data_augmentation(inputs)\n\n    # 0-255 a 0-1.\n    x = layers.Rescaling(1./255)(x)\n\n    # Convoluciones\n    \n    # Buscamos detalles simples como bordes o esquinas.\n    x = layers.Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x) # Esto ayuda a estabilizar el aprendizaje\n    x = layers.MaxPooling2D((2, 2))(x) # Reducimos la imagen a la mitad, 32x32\n\n    # Buscamos texturas o formas más definidas.\n    x = layers.Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = layers.MaxPooling2D((2, 2))(x) # Bajamos a 16x16\n\n    x = layers.Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n    x = layers.MaxPooling2D((2, 2))(x) # Bajamos a 8x8\n\n    x = layers.Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n    x = layers.MaxPooling2D((2, 2))(x) # Bajamos a 4x4\n\n    # Los datos a un vector \n    x = layers.Flatten()(x)\n    x = layers.Dense(512, activation='relu')(x)\n    \n    # Hacemos dropout para apagar la mitad de las neuronas al azar y evitar overfitting\n    x = layers.Dropout(0.5)(x) \n    \n    # 10 outputs\n    outputs = layers.Dense(10, activation='softmax')(x)\n\n    model = keras.Model(inputs=inputs, outputs=outputs, name=\"Custom_CNN\")\n    return model\n\ncnn_model = build_cnn_model_without_N()\ncnn_model.summary()\n\ncnn_model.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy', SparseF1Score(average='weighted', name='f1_score')]\n)\n\ncallbacks_list_cnn = [\n    callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=10, \n        restore_best_weights=True,\n        verbose=1\n    )\n]\n\nprint(\"\\nCNN\")\nstart_time_cnn = time.time()\n\nhistory_cnn_without_normalization = cnn_model.fit(\n    train_ds,\n    epochs=50, # no deberia llegar a hacer los 50 por el early stop\n    validation_data=val_ds,\n    callbacks=callbacks_list_cnn,\n    verbose=1\n)\n\nend_time_cnn = time.time()\ncnn_training_time = end_time_cnn - start_time_cnn\nprint(f\"\\nTiempo de entrenamiento CNN: {cnn_training_time:.2f} s\")","metadata":{"execution":{"iopub.status.busy":"2025-12-30T20:34:55.330139Z","iopub.execute_input":"2025-12-30T20:34:55.330533Z","iopub.status.idle":"2025-12-30T20:37:16.007585Z","shell.execute_reply.started":"2025-12-30T20:34:55.330505Z","shell.execute_reply":"2025-12-30T20:37:16.006905Z"},"papermill":{"duration":100.26149,"end_time":"2025-12-13T19:12:00.175547","exception":false,"start_time":"2025-12-13T19:10:19.914057","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Sin dropout","metadata":{"papermill":{"duration":0.429034,"end_time":"2025-12-13T19:12:01.048094","exception":false,"start_time":"2025-12-13T19:12:00.619060","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def build_cnn_model_without_dropout():\n    # evitamos memorizar el dataset con data augmentation \n    # Giramos y hacemos zoom aleatorio a las imagenes mientras entrena\n    data_augmentation = models.Sequential([\n        layers.RandomFlip(\"horizontal_and_vertical\"),\n        layers.RandomRotation(0.2),\n        layers.RandomZoom(0.1),\n    ], name=\"data_augmentation\")\n    \n    inputs = layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3)) # 64x64 pixeles, 3 canales RGB\n    \n    x = data_augmentation(inputs)\n\n    # 0-255 a 0-1.\n    x = layers.Rescaling(1./255)(x)\n\n    # Convoluciones\n    \n    # Buscamos detalles simples como bordes o esquinas.\n    x = layers.Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x) # Esto ayuda a estabilizar el aprendizaje\n    x = layers.MaxPooling2D((2, 2))(x) # Reducimos la imagen a la mitad, 32x32\n\n    # Buscamos texturas o formas más definidas.\n    x = layers.Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D((2, 2))(x) # Bajamos a 16x16\n\n    x = layers.Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D((2, 2))(x) # Bajamos a 8x8\n\n    x = layers.Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D((2, 2))(x) # Bajamos a 4x4\n\n    # Los datos a un vector \n    x = layers.Flatten()(x)\n    x = layers.Dense(512, activation='relu')(x)    \n    \n    # 10 outputs\n    outputs = layers.Dense(10, activation='softmax')(x)\n\n    model = keras.Model(inputs=inputs, outputs=outputs, name=\"Custom_CNN\")\n    return model\n\ncnn_model = build_cnn_model_without_dropout()\ncnn_model.summary()\n\ncnn_model.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy', SparseF1Score(average='weighted', name='f1_score')]\n)\n\ncallbacks_list_cnn = [\n    callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=10, \n        restore_best_weights=True,\n        verbose=1\n    )\n]\n\nprint(\"\\nCNN\")\nstart_time_cnn = time.time()\n\nhistory_cnn_without_dropout = cnn_model.fit(\n    train_ds,\n    epochs=50, # no deberia llegar a hacer los 50 por el early stop\n    validation_data=val_ds,\n    callbacks=callbacks_list_cnn,\n    verbose=1\n)\n\nend_time_cnn = time.time()\ncnn_training_time = end_time_cnn - start_time_cnn\nprint(f\"\\nTiempo de entrenamiento CNN: {cnn_training_time:.2f} s\")","metadata":{"execution":{"iopub.status.busy":"2025-12-30T20:37:16.008937Z","iopub.execute_input":"2025-12-30T20:37:16.009567Z","iopub.status.idle":"2025-12-30T20:40:01.134850Z","shell.execute_reply.started":"2025-12-30T20:37:16.009538Z","shell.execute_reply":"2025-12-30T20:40:01.134084Z"},"papermill":{"duration":140.769076,"end_time":"2025-12-13T19:14:22.244644","exception":false,"start_time":"2025-12-13T19:12:01.475568","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Gráficas","metadata":{"papermill":{"duration":0.619436,"end_time":"2025-12-13T19:14:23.392326","exception":false,"start_time":"2025-12-13T19:14:22.772890","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#Original para comparar\nplot_history(history_cnn_ES, \"Custom CNN\")","metadata":{"execution":{"iopub.status.busy":"2025-12-30T20:40:01.137990Z","iopub.execute_input":"2025-12-30T20:40:01.138340Z","iopub.status.idle":"2025-12-30T20:40:01.587849Z","shell.execute_reply.started":"2025-12-30T20:40:01.138314Z","shell.execute_reply":"2025-12-30T20:40:01.587223Z"},"papermill":{"duration":1.072058,"end_time":"2025-12-13T19:14:24.985683","exception":false,"start_time":"2025-12-13T19:14:23.913625","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_history(history_cnn_without_DA, \"Custom CNN sin Data Augmentation\")\n","metadata":{"execution":{"iopub.status.busy":"2025-12-30T20:40:01.588703Z","iopub.execute_input":"2025-12-30T20:40:01.588905Z","iopub.status.idle":"2025-12-30T20:40:01.986711Z","shell.execute_reply.started":"2025-12-30T20:40:01.588885Z","shell.execute_reply":"2025-12-30T20:40:01.986064Z"},"papermill":{"duration":1.050388,"end_time":"2025-12-13T19:14:26.657875","exception":false,"start_time":"2025-12-13T19:14:25.607487","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Vemos que al eliminar el data augmentation (y por tanto las rotaciones y los zooms aleatorios), la red se aprende de memoria las características en vez de generalizar, lo que lleva a un mayor puntuaje en training y menor en validation (overfitting). Además podemos ver que los picos han aumentado, posiblemente a causa del gran overfitting que tiene el modelo sin aplicar un data augmentation.","metadata":{"papermill":{"duration":0.534878,"end_time":"2025-12-13T19:14:27.728562","exception":false,"start_time":"2025-12-13T19:14:27.193684","status":"completed"},"tags":[]}},{"cell_type":"code","source":"plot_history(history_cnn_without_normalization, \"Custom CNN sin Normalización\")","metadata":{"execution":{"iopub.status.busy":"2025-12-30T20:40:01.987587Z","iopub.execute_input":"2025-12-30T20:40:01.987876Z","iopub.status.idle":"2025-12-30T20:40:02.417740Z","shell.execute_reply.started":"2025-12-30T20:40:01.987842Z","shell.execute_reply":"2025-12-30T20:40:02.417067Z"},"papermill":{"duration":1.024946,"end_time":"2025-12-13T19:14:29.391666","exception":false,"start_time":"2025-12-13T19:14:28.366720","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Vemos que sin aplicar Batch Normalization el modelo se vuelve muy inestable en sus predicciones de validation. Esto puede deberse a que los pesos de la red cambian demasiado bruscamente, haciendo que el gradiente sea inestable. En adición, el modelo tarda más en aprender, posiblemente por estos cambios tan bruscos de parámetros.","metadata":{"papermill":{"duration":0.616744,"end_time":"2025-12-13T19:14:30.543263","exception":false,"start_time":"2025-12-13T19:14:29.926519","status":"completed"},"tags":[]}},{"cell_type":"code","source":"plot_history(history_cnn_without_dropout, \"Custom CNN sin Dropout\")","metadata":{"execution":{"iopub.status.busy":"2025-12-30T20:40:02.418524Z","iopub.execute_input":"2025-12-30T20:40:02.418729Z","iopub.status.idle":"2025-12-30T20:40:02.852570Z","shell.execute_reply.started":"2025-12-30T20:40:02.418708Z","shell.execute_reply":"2025-12-30T20:40:02.851944Z"},"papermill":{"duration":1.018722,"end_time":"2025-12-13T19:14:32.085465","exception":false,"start_time":"2025-12-13T19:14:31.066743","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_history(history_cnn, \"Custom CNN sin Early Stopping\", epoch_parada_CNN)","metadata":{"execution":{"iopub.status.busy":"2025-12-30T20:40:02.853331Z","iopub.execute_input":"2025-12-30T20:40:02.853537Z","iopub.status.idle":"2025-12-30T20:40:03.296432Z","shell.execute_reply.started":"2025-12-30T20:40:02.853517Z","shell.execute_reply":"2025-12-30T20:40:03.295720Z"},"papermill":{"duration":1.145959,"end_time":"2025-12-13T19:14:33.762766","exception":false,"start_time":"2025-12-13T19:14:32.616807","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Con estas gráficas podemos validar que el no desactivar neuronas provoca un mayor overfitting (pues hay mayor separación entre validation y training), ya que el modelo se fija más en ciertas características en vez de generalizar. Por otro lado, podemos observar que al activar Early Stopping, el modelo no habria llegado a un estado en el que es mas preciso, a coste de un mayor overfitting.","metadata":{"papermill":{"duration":0.524085,"end_time":"2025-12-13T19:14:34.824568","exception":false,"start_time":"2025-12-13T19:14:34.300483","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"---\n## 5. Transfer Learning (Estado del Arte)\n\nEn esta sección, utilizaremos una red preentrenada en ImageNet para aprovechar el conocimiento extraído de millones de imágenes. Dado el tamaño de entrada ($64 \\times 64$), buscamos arquitecturas eficientes que no reduzcan excesivamente la dimensionalidad espacial.\n\nDebes construir un nuevo modelo siguiendo estas **pautas**:\n1.  Integra la misma capa de aumento de datos definida en la sección anterior.\n2.  Importa una arquitectura robusta (como `ResNet50V2` o `EfficientNetV2B0`) con pesos de `imagenet`. Recuerda configurar `include_top=False` y definir el `input_shape` correcto.\n3.  Congela los pesos del modelo base (Feature Extraction) y añade tu(s) propia(s) capa(s) completamente contectada(s) para clasificación. Se recomienda usar `GlobalAveragePooling2D` para conectar la base convolucional con la capa de salida.\n\n**Tarea:**\n1.  Construye, compila y entrena el modelo.\n2.  Guarda el mejor modelo generado como `best_transfer.keras`.\n3.  Analiza si el uso de modelos preentrenados justifica la mejora en precisión respecto a tu CNN \"artesanal\", considerando el tiempo de entrenamiento y el número de parámetros.\n4.  Se valorará el estudio de descongelar los últimos bloques del modelo base para realizar un *Fine-Tuning* con una tasa de aprendizaje reducida (ej. $1e-5$).","metadata":{"papermill":{"duration":0.527626,"end_time":"2025-12-13T19:14:35.876055","exception":false,"start_time":"2025-12-13T19:14:35.348429","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### Función genérica para agregar más modelos","metadata":{}},{"cell_type":"code","source":"data_augmentation = keras.Sequential([\n    layers.RandomFlip(\"horizontal_and_vertical\"),\n    layers.RandomRotation(0.2),\n    layers.RandomZoom(0.1),\n])\n\ndef build_generic_model(model_name, img_height, img_width, num_classes=10):\n    inputs = keras.Input(shape=(img_height, img_width, 3))\n    \n    x = data_augmentation(inputs)\n    \n    # Selección del modelo base y Preprocesamiento específico\n    if model_name == 'EfficientNetV2B0':\n        # EfficientNetV2 normaliza internamente. Espera [0, 255]\n        base_model = tf.keras.applications.EfficientNetV2B0(\n            include_top=False, weights='imagenet', input_shape=(img_height, img_width, 3)\n        )\n    \n    elif model_name == 'ConvNeXtTiny':\n        base_model = tf.keras.applications.ConvNeXtTiny(\n            include_top=False, weights='imagenet', input_shape=(img_height, img_width, 3)\n        )\n        \n    elif model_name == 'ResNet50V2':\n        # como ResNetV2 espera inputs entre [-1, 1] necesitamos reescalar.\n        x = layers.Rescaling(1./127.5, offset=-1)(x)\n        base_model = tf.keras.applications.ResNet50V2(\n            include_top=False, weights='imagenet', input_shape=(img_height, img_width, 3)\n        )\n        \n    else:\n        raise ValueError(f\"Modelo {model_name} no soportado\")\n\n    # Freezing inicial\n    base_model.trainable = False\n    \n    # training=False para mantener las estadísticas de BatchNormalization de ImageNet\n    x = base_model(x, training=False)\n    \n    # Cabezal de clasificación (Top)\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Dropout(0.2)(x)\n    outputs = layers.Dense(num_classes, activation='softmax')(x)\n    \n    model = keras.Model(inputs, outputs, name=f\"Transfer_{model_name}\")\n    return model, base_model","metadata":{"execution":{"iopub.status.busy":"2025-12-30T20:59:54.291342Z","iopub.execute_input":"2025-12-30T20:59:54.291631Z","iopub.status.idle":"2025-12-30T20:59:54.306310Z","shell.execute_reply.started":"2025-12-30T20:59:54.291607Z","shell.execute_reply":"2025-12-30T20:59:54.305589Z"},"papermill":{"duration":489.279317,"end_time":"2025-12-13T19:22:45.769771","exception":false,"start_time":"2025-12-13T19:14:36.490454","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_full_pipeline(model, base_model, train_ds, val_ds, class_weights, early_stop=True ,epochs_warmup=25, epochs_fine=50):\n    \n    print(f\"\\n>>> Iniciando entrenamiento para: {model.name}\")\n    \n    # --- PREPARACIÓN DE CALLBACKS ---\n    checkpoint_path = f\"best_{model.name}.keras\"\n    \n    checkpoint_callback = callbacks.ModelCheckpoint(\n        checkpoint_path, \n        monitor='val_accuracy', \n        save_best_only=True, \n        verbose=0\n    )\n    \n    #FEATURE EXTRACTION (Congelado) ---\n    model.compile(\n        optimizer='adam',\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy',SparseF1Score(average='weighted', name='f1_score')]\n    )\n\n    if (early_stop):\n        callbacks_transfer = [\n            callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),\n            checkpoint_callback\n        ]\n    else:\n        callbacks_transfer = [\n            checkpoint_callback\n        ]\n    \n    print(f\"   Congelado (Guardando en {checkpoint_path})...\")\n    start_time = time.time()\n    \n    history_tl = model.fit(\n        train_ds,\n        epochs=epochs_warmup,\n        validation_data=val_ds,\n        callbacks=callbacks_transfer,\n        class_weight=class_weights,\n        verbose=1\n    )\n\n    fine_tuning_at_epoch = len(history_tl.history['val_accuracy']) - 1\n    \n    # FINE TUNING \n    print(\"   Descongelado (Fine Tuning)...\")\n    base_model.trainable = True\n\n    # Definimos cuántas capas dejar entrenables, si descongelamos todas nos quedamos sin VRAM\n    fine_tune_at = len(base_model.layers) - 40\n\n    for layer in base_model.layers[:fine_tune_at]:\n        layer.trainable = False\n    \n    model.compile(\n        optimizer=keras.optimizers.Adam(1e-5), # Learning rate muy bajo\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy',SparseF1Score(average='weighted', name='f1_score')]\n    )\n    \n    if (early_stop):\n        callbacks_finetune = [\n            callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),\n            checkpoint_callback\n        ]\n    else:\n        callbacks_finetune = [\n            checkpoint_callback\n        ]\n    \n    history_ft = model.fit(\n        train_ds,\n        epochs=epochs_fine,\n        initial_epoch=history_tl.epoch[-1],\n        validation_data=val_ds,\n        callbacks=callbacks_finetune,\n        class_weight=class_weights,\n        verbose=1\n    )\n    \n    total_time = time.time() - start_time\n    print(f\">>> Tiempo total {model.name}: {total_time:.2f} s\")\n    \n    # Combinamos los hystories de los dos (tl y ft)\n    combined_dict = {}\n    for key in history_tl.history.keys():\n        # get para evitar errores si alguna métrica falta en la segunda fase\n        list_1 = history_tl.history.get(key, [])\n        list_2 = history_ft.history.get(key, [])\n        combined_dict[key] = list_1 + list_2\n        \n    # Historial, Modelo, Tiempo\n    return combined_dict, model, total_time, fine_tuning_at_epoch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T20:59:58.687481Z","iopub.execute_input":"2025-12-30T20:59:58.687775Z","iopub.status.idle":"2025-12-30T20:59:58.697558Z","shell.execute_reply.started":"2025-12-30T20:59:58.687749Z","shell.execute_reply":"2025-12-30T20:59:58.696840Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Wrapper para ejecutar los modelos","metadata":{}},{"cell_type":"code","source":"model_candidates = [\n    'EfficientNetV2B0', \n    'ConvNeXtTiny', \n    'ResNet50V2'\n                   ]\n\n# Diccionarios para guardar resultados, tiempos y parámetros para la comparativa\nresults_ES = {}\ntimes_ES = {}\nparams_ES = {}\nfine_tune_starts_ES = {}\nmodels_ES = {}\n\nprint(f\"Iniciando entrenamiento de modelos: {model_candidates}\")\n\nfor model_name in model_candidates:\n    print(f\"\\n--- Procesando modelo: {model_name} ---\")\n    \n    # Construcción del modelo (usamos tu snippet)\n    model, base_model = build_generic_model(model_name, IMG_HEIGHT, IMG_WIDTH, num_classes=10)\n    \n    # Guardamos conteo de parámetros para la tabla\n    params_ES[model_name] = model.count_params()\n    \n    # Entrenamiento\n    hist, trained_model, execution_time, fine_tune_start = train_full_pipeline(\n        model, \n        base_model, \n        train_ds, \n        val_ds, \n        class_weights\n    )\n    \n    # Guardar resultados\n    results_ES[model_name] = hist\n    times_ES[model_name] = execution_time\n    fine_tune_starts_ES[model_name] = fine_tune_start\n    models_ES[model_name] = trained_model\n    \n    # Limpiar memoria de GPU entre modelos\n    keras.backend.clear_session()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T20:47:52.735291Z","iopub.execute_input":"2025-12-30T20:47:52.735592Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Transfer Learning sin Early Stopping","metadata":{"papermill":{"duration":0.87462,"end_time":"2025-12-13T19:22:47.411027","exception":false,"start_time":"2025-12-13T19:22:46.536407","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Diccionarios para guardar resultados, tiempos y parámetros para la comparativa\nresults = {}\ntimes = {}\nparams = {}\nfine_tune_starts = {}\nmodels = {}\n\nprint(f\"Iniciando entrenamiento SIN EARLY STOP de modelos: {model_candidates}\")\n\nfor model_name in model_candidates:\n    print(f\"\\n--- Procesando modelo: {model_name} ---\")\n    \n    # Construcción del modelo (usamos tu snippet)\n    model, base_model = build_generic_model(model_name, IMG_HEIGHT, IMG_WIDTH, num_classes=10)\n    \n    # Guardamos conteo de parámetros para la tabla\n    params[model_name] = model.count_params()\n    \n    # Entrenamiento\n    hist, trained_model, execution_time, fine_tune_start = train_full_pipeline(\n        model, \n        base_model, \n        train_ds, \n        val_ds, \n        class_weights,\n        early_stop = False\n    )\n    \n    # Guardar resultados\n    results[model_name] = hist\n    times[model_name] = execution_time\n    fine_tune_starts[model_name] = fine_tune_start\n    models[model_name] = trained_model\n    \n    # Limpiar memoria de GPU entre modelos\n    keras.backend.clear_session()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T21:00:08.230445Z","iopub.execute_input":"2025-12-30T21:00:08.230735Z","iopub.status.idle":"2025-12-30T21:05:23.402297Z","shell.execute_reply.started":"2025-12-30T21:00:08.230709Z","shell.execute_reply":"2025-12-30T21:05:23.400172Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Visualizar graficas y comparativa final","metadata":{"papermill":{"duration":1.264056,"end_time":"2025-12-13T19:36:56.573871","exception":false,"start_time":"2025-12-13T19:36:55.309815","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def plt_transfer_learning(models_history,fine_tune_starts): \n    \"\"\"\n    Recibe un diccionario con:\n    - Key: Nombre del modelo (str)\n    - Value: Objeto history del entrenamiento\n    Ejemplo: plt_transfer_learning({'VGG16': history_vgg, 'ResNet': history_resnet})\n    \"\"\"\n    \n    # Iteramos sobre cada modelo en el diccionario\n    for model_name, history_tl in models_history.items():\n        \n        # Extraemos las métricas directamente del historial actual\n        acc = history_tl['accuracy']\n        val_acc = history_tl['val_accuracy']\n        loss = history_tl['loss']\n        val_loss = history_tl['val_loss']\n        f1 = history_tl['f1_score']\n        val_f1 = history_tl['val_f1_score']\n\n        plt.figure(figsize=(12, 10))\n        \n        # --- Gráfica de Accuracy ---\n        plt.subplot(2, 2, 1)\n        plt.plot(acc, label='Training accuracy')\n        plt.plot(val_acc, label='Validation accuracy')\n        plt.plot([fine_tune_starts[model_name], fine_tune_starts[model_name]], plt.ylim(), label='Inicio Fine Tuning', ls='--') \n        plt.legend(loc='lower right')\n        plt.title(f'Evolución de Accuracy: {model_name}') # Nombre del modelo incluido\n        plt.grid(True)\n\n        # --- Gráfica de F1-Score ---\n        plt.subplot(2, 2, 3)\n        plt.plot(f1, label='Training F1-score')\n        plt.plot(val_f1, label='Validation F1-score')\n        plt.plot([fine_tune_starts[model_name], fine_tune_starts[model_name]], plt.ylim(), label='Inicio Fine Tuning', ls='--') \n        plt.legend(loc='lower right')\n        plt.title(f'Evolución de F1-score: {model_name}') # Nombre del modelo incluido\n        plt.grid(True)\n\n        # --- Gráfica de Loss ---\n        plt.subplot(2, 2, 2)\n        plt.plot(loss, label='Pérdida en training')\n        plt.plot(val_loss, label='Pérdida en validation')\n        plt.plot([fine_tune_starts[model_name], fine_tune_starts[model_name]], plt.ylim(), label='Inicio Fine Tuning', ls='--')\n        plt.legend(loc='upper right')\n        plt.title(f'Evolución de la Pérdida: {model_name}') # Nombre del modelo incluido\n        plt.grid(True)\n        \n        plt.tight_layout() # Para que no se solapen textos si hay muchas gráficas\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2025-12-30T20:44:40.956901Z","iopub.execute_input":"2025-12-30T20:44:40.957859Z","iopub.status.idle":"2025-12-30T20:44:40.966313Z","shell.execute_reply.started":"2025-12-30T20:44:40.957816Z","shell.execute_reply":"2025-12-30T20:44:40.965544Z"},"papermill":{"duration":1.28545,"end_time":"2025-12-13T19:36:59.019243","exception":false,"start_time":"2025-12-13T19:36:57.733793","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Intentamos recuperar métricas de los modelos anteriores (si existen en memoria)\ntry:\n    mlp_acc = max(history_mlp.history['val_accuracy'])\n    cnn_ES_acc = max(history_cnn_ES.history['val_accuracy'])\n    cnn_acc = max(history_cnn.history['val_accuracy'])\n    mlp_f1 = max(history_mlp.history['val_f1_score'])\n    cnn_ES_f1 = max(history_cnn_ES.history['val_f1_score'])\n    cnn_f1 = max(history_cnn.history['val_f1_score'])\n    mlp_par = mlp_model.count_params()\n    cnn_ES_par = cnn_model.count_params()\n    cnn_par = cnn_model.count_params()\n\n    mlp_acc_ES = max(history_mlp_ES.history['val_accuracy'])\n    mlp_f1_ES = max(history_mlp_ES.history['val_f1_score'])\n    mlp_par_ES = mlp_model_ES.count_params()\nexcept NameError:\n    # Valores por defecto si no existen\n    mlp_acc, cnn_ES_acc, cnn_acc, mlp_f1, cnn_f1_ES, cnn_f1, mlp_par, cnn_par_ES, cnn_par = 0, 0, 0, 0, 0, 0, 0, 0, 0\n    mlp_acc_ES, mlp_f1_ES, cnn_ES_f1, mlp_par_ES, cnn_ES_par = 0, 0, 0, 0, 0\n\nprint(\"\\n\" + \"=\"*85)\nprint(\"      COMPARATIVA DE ARQUITECTURAS\")\nprint(\"=\"*85)\nprint(f\"{'Modelo':<25} | {'Parámetros':<12} | {'Val Acc':<10} | {'Val F1':<10} | {'Tiempo(s)':<10}\")\nprint(\"-\" * 85)\n# Filas de otros modelos \nprint(f\"{'MLP (Básico)':<25} | {mlp_par:<12,} | {mlp_acc:.4f}   | {mlp_f1:.4f}     | {mlp_training_time if 'mlp_training_time' in locals() else 0:.1f}\")\nprint(f\"{'MLP (con Early Stop)':<25} | {mlp_par_ES:<12,} | {mlp_acc_ES:.4f}   | {mlp_f1_ES:.4f}     | {mlp_ES_training_time if 'mlp_ES_training_time' in locals() else 0:.1f}\")\nprint(f\"{'CNN':<25} | {cnn_par:<12,} | {cnn_acc:.4f}   | {cnn_f1:.4f}     | {cnn_training_time if 'cnn_training_time' in locals() else 0:.1f}\")\nprint(f\"{'CNN (Con Early Stop)':<25} | {cnn_ES_par:<12,} | {cnn_ES_acc:.4f}   | {cnn_ES_f1:.4f}     | {cnn_ES_training_time if 'cnn_ES_training_time' in locals() else 0:.1f}\")\n\nprint(\"-\" * 85)\n# Filas dinámicas para los modelos de Transfer Learning\nfor name, hist in results_ES.items():\n    tl_acc = max(hist['val_accuracy'])\n    tl_f1 = max(hist['val_f1_score'])\n    tl_par = params_ES[name]\n    tl_time = times_ES.get(name, 0)\n    \n    print(f\"{'TL: ' + name+' con ES':<25} | {tl_par:<12,} | {tl_acc:.4f}   | {tl_f1:.4f}     | {tl_time:.1f}\")\n\nfor name, hist in results.items():\n    tl_acc = max(hist['val_accuracy'])\n    tl_f1 = max(hist['val_f1_score'])\n    tl_par = params_ES[name]\n    tl_time = times_ES.get(name, 0)\n    \n    print(f\"{'TL: ' + name:<25} | {tl_par:<12,} | {tl_acc:.4f}   | {tl_f1:.4f}     | {tl_time:.1f}\")\n\nprint(\"-\" * 85)","metadata":{"execution":{"iopub.status.busy":"2025-12-30T21:09:01.787041Z","iopub.execute_input":"2025-12-30T21:09:01.787627Z","iopub.status.idle":"2025-12-30T21:09:01.798490Z","shell.execute_reply.started":"2025-12-30T21:09:01.787593Z","shell.execute_reply":"2025-12-30T21:09:01.797691Z"},"papermill":{"duration":1.288515,"end_time":"2025-12-13T19:37:01.460215","exception":false,"start_time":"2025-12-13T19:37:00.171700","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Gráficas con Early Stopping","metadata":{"papermill":{"duration":1.16012,"end_time":"2025-12-13T19:37:03.882668","exception":false,"start_time":"2025-12-13T19:37:02.722548","status":"completed"},"tags":[]}},{"cell_type":"code","source":"plt_transfer_learning(results_ES,fine_tune_starts_ES)","metadata":{"execution":{"iopub.status.busy":"2025-12-30T20:44:48.510512Z","iopub.execute_input":"2025-12-30T20:44:48.510807Z","iopub.status.idle":"2025-12-30T20:44:48.980656Z","shell.execute_reply.started":"2025-12-30T20:44:48.510782Z","shell.execute_reply":"2025-12-30T20:44:48.980016Z"},"papermill":{"duration":1.583775,"end_time":"2025-12-13T19:37:06.723554","exception":false,"start_time":"2025-12-13T19:37:05.139779","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Gráficas SIN Early Stopping","metadata":{"papermill":{"duration":1.261146,"end_time":"2025-12-13T19:37:09.275819","exception":false,"start_time":"2025-12-13T19:37:08.014673","status":"completed"},"tags":[]}},{"cell_type":"code","source":"plt_transfer_learning(results,fine_tune_starts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T21:06:51.809889Z","iopub.execute_input":"2025-12-30T21:06:51.810868Z","iopub.status.idle":"2025-12-30T21:06:53.109647Z","shell.execute_reply.started":"2025-12-30T21:06:51.810817Z","shell.execute_reply":"2025-12-30T21:06:53.108908Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"La mayor diferencia de estas gráficas frente a nuestra CNN propia es que empieza con un accuracy bastante más alto. Esto puede deberse al hehco de que el modelo ya viene pre-entrenado, y por tanto ya sabe distinguir ciertas formas y texturas. Además no tiene nada de overfitting pues el accuracy de validation es mayor al de train (y su pérdida menor). Al hacer el fine tuning se ve que mejora drásticamente tanto en validation como en train, posiblemente por adaptar los valores previamente congelados a las características de nuestro dataset. Por ultimo, tenemos el mismo caso que antes, al activar la opción Early Stopping el modelo tendra algo menos de precision, sin embargo conforme avanzan las epocas vemos aun más overfitting en el modelo. Antes de que saltara el ES, el modelo no hacia practicamente overfitting, como hemos dicho antes, sin embargo tras el fine tunning y ES combinados el resultado final no sería tan ajustado para otros dataset, dandole menos utilidad real.\n\nAunque este modelo consigue una mejor accuracy que nuestra CNN, también tarda el doble y usa bastantes más parámetros, por lo que en la mayoría de casos nuestra CNN sería un buen compromiso entre recursos y rendimiento. En comparación con el MLP, este Transfer Learning es mucho mejor, pues sigue teniendo menos que el MLP mientras consigue un rendimiento bastante superior.","metadata":{"papermill":{"duration":1.155768,"end_time":"2025-12-13T19:37:14.515256","exception":false,"start_time":"2025-12-13T19:37:13.359488","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"---\n## 6. (Opcional) Nuevas Fronteras: Vision Transformers (ViT)\n\nMientras que las CNN han dominado la visión por computador durante una década, los **Transformers** (originalmente diseñados para texto) han irrumpido con fuerza en el campo.\n\nA diferencia de una CNN, que mira píxeles vecinos, un **Vision Transformer (ViT)** divide la imagen en \"parches\" (por ejemplo, cuadrados de $16 \\times 16$) y procesa la relación de cada parche con todos los demás simultáneamente mediante mecanismos de **Atención**.\n\n**Desafío Técnico:**\nLos ViT son modelos muy pesados y requieren una resolución de entrada específica (generalmente $224 \\times 224$). Además, su integración en Keras requiere adaptadores especiales.\n\n**Tarea (Bonus):**\n1.  Instala la librería `transformers` de Hugging Face.\n2.  Utiliza la clase `ViTWrapper` proporcionada abajo para cargar un modelo base de Google (`vit-base-patch16-224`).\n3.  **Completa la función `build_vit_classifier`**:\n    * Añade tu capa de **Data Augmentation** al inicio.\n    * Observa cómo adaptamos la imagen (`Resizing`, `Rescaling` y `Permute`) para que sea compatible con el modelo de Google.\n    * Añade tu(s) propia(s) capa(s) completamente contectada(s) de clasificación al final (Capas Densas y de Salida).\n4.  Entrena por pocas épocas y compara los resultados (y el tiempo de ejecución) con tu mejor CNN.","metadata":{"papermill":{"duration":1.269134,"end_time":"2025-12-13T19:37:17.037746","exception":false,"start_time":"2025-12-13T19:37:15.768612","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### Limpieza de variables y basura para ejecutar en local","metadata":{"papermill":{"duration":1.262864,"end_time":"2025-12-13T19:37:19.479681","exception":false,"start_time":"2025-12-13T19:37:18.216817","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Limpiamos sesiones anteriores para liberar VRAM de la GPU (en nuestros portátiles es necesario)\ntf.keras.backend.clear_session()\ngc.collect()\n\nUSE_REDUCED_BATCH_SIZE = True \n\nif USE_REDUCED_BATCH_SIZE:\n    BATCH_SIZE_VIT = 16\n    print(f\"Usando Batch Size REDUCIDO: {BATCH_SIZE_VIT}\")\nelse:\n    BATCH_SIZE_VIT = 64\n    print(f\"Usando Batch Size: {BATCH_SIZE_VIT}\")","metadata":{"execution":{"iopub.status.busy":"2025-12-30T20:44:36.809061Z","iopub.status.idle":"2025-12-30T20:44:36.809335Z","shell.execute_reply.started":"2025-12-30T20:44:36.809205Z","shell.execute_reply":"2025-12-30T20:44:36.809224Z"},"papermill":{"duration":2.137871,"end_time":"2025-12-13T19:37:22.870002","exception":false,"start_time":"2025-12-13T19:37:20.732131","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Carga de train y validation","metadata":{"papermill":{"duration":1.157926,"end_time":"2025-12-13T19:37:25.287147","exception":false,"start_time":"2025-12-13T19:37:24.129221","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_ds_vit = tf.keras.utils.image_dataset_from_directory(\n    data_dir,\n    validation_split=0.15,\n    subset=\"training\",\n    seed=2025,\n    image_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=BATCH_SIZE_VIT,\n    label_mode='int'\n)\n\nval_ds_vit = tf.keras.utils.image_dataset_from_directory(\n    data_dir,\n    validation_split=0.15,\n    subset=\"validation\",\n    seed=2025,\n    image_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=BATCH_SIZE_VIT,\n    label_mode='int'\n)\n# Optimización de carga\nAUTOTUNE = tf.data.AUTOTUNE\ntrain_ds_vit = train_ds_vit.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\nval_ds_vit = val_ds_vit.cache().prefetch(buffer_size=AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2025-12-30T20:44:36.810481Z","iopub.status.idle":"2025-12-30T20:44:36.810735Z","shell.execute_reply.started":"2025-12-30T20:44:36.810625Z","shell.execute_reply":"2025-12-30T20:44:36.810639Z"},"papermill":{"duration":12.44323,"end_time":"2025-12-13T19:37:38.979147","exception":false,"start_time":"2025-12-13T19:37:26.535917","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Funciones de ViT","metadata":{"papermill":{"duration":1.250512,"end_time":"2025-12-13T19:37:41.373226","exception":false,"start_time":"2025-12-13T19:37:40.122714","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class ViTWrapper(layers.Layer):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        # Cargamos el modelo base pre-entrenado\n        # hemos añadido from_pt=True para cargar pesos originales de PyTorch\n        # y evitar los errores de lectura en TensorFlow.\n        self.vit = TFViTModel.from_pretrained('google/vit-base-patch16-224', from_pt=True)\n        self.vit.trainable = False # Congelamos pesos base\n        \n    def call(self, inputs):\n        # El modelo espera 'pixel_values'\n        # Retornamos 'pooler_output' que es la representación vectorial de la imagen\n        return self.vit(pixel_values=inputs).pooler_output\n\ndef build_vit_classifier():\n    inputs = keras.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n    \n    # 1. Data Augmentation\n    x = layers.RandomFlip(\"horizontal_and_vertical\")(inputs)\n    x = layers.RandomRotation(0.2)(x)\n    x = layers.RandomZoom(0.1)(x)\n    \n    # 2. Adaptación de Dimensionalidad (Necesario para ViT)\n    # Redimensionamos a 224x224 (resolución nativa de ViT)\n    x = layers.Resizing(224, 224)(x)\n    # Escalamos los píxeles entre -1 y 1\n    x = layers.Rescaling(1./127.5, offset=-1)(x)\n    # Transponemos canales a (Canales, Alto, Ancho) para compatibilidad con Hugging Face\n    x = layers.Permute((3, 1, 2))(x)\n    \n    # 3. Bloque Transformer\n    x = ViTWrapper()(x)\n    \n    # 4. Clasificación\n    x = layers.Dense(256, activation='relu')(x)\n    x = layers.Dropout(0.2)(x) # intentamos evitar overfitting\n    outputs = layers.Dense(10, activation='softmax')(x)\n    \n    return keras.Model(inputs, outputs, name=\"ViT_Classifier\")","metadata":{"execution":{"iopub.status.busy":"2025-12-30T20:44:36.812342Z","iopub.status.idle":"2025-12-30T20:44:36.812676Z","shell.execute_reply.started":"2025-12-30T20:44:36.812503Z","shell.execute_reply":"2025-12-30T20:44:36.812525Z"},"papermill":{"duration":1.161082,"end_time":"2025-12-13T19:37:43.777465","exception":false,"start_time":"2025-12-13T19:37:42.616383","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vit_model = build_vit_classifier()\nvit_model.summary()\n\n\nvit_model.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy', SparseF1Score(average='weighted', name='f1_score')]\n)\n\ncallbacks_vit = [\n    callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1),\n    callbacks.ModelCheckpoint('best_vit.keras', monitor='val_accuracy', save_best_only=True, verbose=0)\n]\n\nprint(f\"\\n--- Iniciando entrenamiento ViT (Batch Size: {BATCH_SIZE_VIT}) ---\")\nstart_time_vit = time.time()\n\n\nhistory_vit = vit_model.fit(\n    train_ds_vit,\n    epochs=10, # Pocas épocas para que tarde menos de 1 hora en local\n    validation_data=val_ds_vit,\n    callbacks=callbacks_vit\n)\n\nend_time_vit = time.time()\nvit_training_time = end_time_vit - start_time_vit\nprint(f\"\\nTiempo total de entrenamiento (ViT): {vit_training_time:.2f} segundos\")","metadata":{"execution":{"iopub.status.busy":"2025-12-30T20:44:36.813668Z","iopub.status.idle":"2025-12-30T20:44:36.813988Z","shell.execute_reply.started":"2025-12-30T20:44:36.813846Z","shell.execute_reply":"2025-12-30T20:44:36.813869Z"},"papermill":{"duration":4084.489973,"end_time":"2025-12-13T20:45:49.516692","exception":false,"start_time":"2025-12-13T19:37:45.026719","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_history(history_vit, \"Vision Transformer (ViT)\")\n\n# Recuperamos las métricas de la CNN asumiendo que siguen en memoria\ncnn_best_val_acc = max(history_cnn.history['val_accuracy'])\ncnn_best_val_f1 = max(history_cnn.history['val_f1_score'])\ncnn_params = cnn_model.count_params()\n\n# Recuperamos métricas del ViT\nvit_best_val_acc = max(history_vit.history['val_accuracy'])\nvit_best_val_f1 = max(history_vit.history['val_f1_score'])\nvit_params = vit_model.count_params()\n\n# tabla comparativa\nprint(\"\\n\" + \"=\"*75)\nprint(\"                         CNN vs ViT\")\nprint(\"=\"*75)\nprint(f\"{'Modelo':<25} | {'Parámetros':<12} | {'Val Acc':<10} | {'Val F1':<10} | {'Tiempo(s)':<10}\")\nprint(\"-\" * 75)\n\n# CNN\nprint(f\"{'CNN (Propia)':<25} | {cnn_par:<12,} | {cnn_best_val_acc:.4f}  | {cnn_best_val_f1:.4f}     | {cnn_training_time:.1f}\")\n\n# ViT\nprint(f\"{'ViT (HuggingFace)':<25} | {vit_params:<12,} | {vit_best_val_acc:.4f}  | {vit_best_val_f1:.4f}     | {vit_training_time:.1f}\")\nprint(\"-\" * 75)\n\n# Grafica comparativa\nplt.figure(figsize=(10, 6))\n\n# Curva CNN\nval_acc_cnn = history_cnn.history['val_accuracy']\nplt.plot(val_acc_cnn, label=f'CNN Propia (Max: {cnn_best_val_acc:.2%})', linestyle='--', linewidth=2)\n\n# Curva ViT\nval_acc_vit = history_vit.history['val_accuracy']\nplt.plot(val_acc_vit, label=f'ViT (Max: {vit_best_val_acc:.2%})', linewidth=2)\n\nplt.title('Comparativa de Validación: Convolución vs Transformer')\nplt.xlabel('Épocas')\nplt.ylabel('Validation Accuracy')\nplt.legend(loc='lower right')\nplt.grid(True, alpha=0.3)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2025-12-30T20:44:36.815813Z","iopub.status.idle":"2025-12-30T20:44:36.816156Z","shell.execute_reply.started":"2025-12-30T20:44:36.815970Z","shell.execute_reply":"2025-12-30T20:44:36.815994Z"},"papermill":{"duration":2.688263,"end_time":"2025-12-13T20:45:54.080501","exception":false,"start_time":"2025-12-13T20:45:51.392238","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Vemos que con este dataset en específico la ViT no obtiene apenas ganancia, a pesar de tardar varias veces más que la CNN. Por otro lado, con el mismo número de épocas sí que obtiene bastante mejor resultado, por lo que es posible que si la dejaramos ejecutarse con el mismo número de épocas que la CNN aumentara la diferencia.\n\nEste aumento de tiempo se entiende al mirar el número de parámetros, muy superior a la CNN y MLP. Es posible que con datasets mayores, con más resolución y complejidad el ViT obtuviese muchos mejores resultados.","metadata":{"papermill":{"duration":1.929643,"end_time":"2025-12-13T20:45:57.803049","exception":false,"start_time":"2025-12-13T20:45:55.873406","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"---\n## 7. Evaluación y Análisis Comparativo\n\nEn esta sección final evaluaremos el mejor modelo y realizaremos un estudio comparativo de todas las arquitecturas probadas.\n\n**Tareas:**\n1.  **Evaluación Cuantitativa:**\n    * Carga el mejor modelo de todos los generados.\n    * Genera el `classification_report` para ver la precisión, recall y F1-score por cada clase.\n    * Visualiza la **Matriz de Confusión**. Identifica los pares de clases más problemáticos.\n\n2.  **Análisis Cualitativo de Errores:**\n    * Visualiza algunas imágenes mal clasificadas. \n    * **Requisito:** Muestra la imagen, la etiqueta real, la etiqueta predicha y el **nivel de confianza** (probabilidad) del modelo en esa predicción errónea. ¿Son errores \"evitables\"?\n\n3.  **Comparativa Final:**\n    * Crea un DataFrame de Pandas comparando al menos MLP, CNN Propia y CNN Transfer Learning (opcional ViT). *En este punto se pueden incluir también comparaciones con modelos con/sin aumento de datos, con/sin dropout, diversas arquitecturas...*\n    * Ejemplo de columnas: `Modelo`, `Accuracy (Val)`, `F1-Score (Macro)`, `Nº Parámetros`, `Nº Parámetros Entrenables`, `Tiempo de Entrenamiento`.\n    * **Conclusión:** Escribe un par de párrafos justificando cuál es la mejor arquitectura considerando el equilibrio entre recursos computacionales (parámetros/tiempo) y precisión.","metadata":{"papermill":{"duration":2.017859,"end_time":"2025-12-13T20:46:01.709598","exception":false,"start_time":"2025-12-13T20:45:59.691739","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def save_metric(model_input, history_input, name, time_input=0):\n    \"\"\"\n    Admite nombres de variables (str) o los objetos directamente.\n    \"\"\"\n    # 1. OBTENER EL MODELO\n    if isinstance(model_input, str):\n        if model_input not in globals(): return None\n        model = globals()[model_input]\n    else:\n        model = model_input\n\n    # 2. OBTENER EL HISTORY\n    if isinstance(history_input, str):\n        if history_input not in globals(): return None\n        history_obj = globals()[history_input]\n    else:\n        history_obj = history_input\n\n    # 3. OBTENER EL TIEMPO\n    if isinstance(time_input, str):\n        time_val = globals().get(time_input, 0)\n    else:\n        time_val = time_input\n\n    # 4. EXTRAER MÉTRICAS\n    try:\n        # Detectamos si es un objeto History de Keras (tiene atributo .history) o un diccionario puro\n        history_dict = history_obj.history if hasattr(history_obj, 'history') else history_obj\n        \n        # Obtenemos máximos\n        val_acc = max(history_dict['val_accuracy'])\n        val_f1 = max(history_dict['val_f1_score'])\n        params = model.count_params()\n        \n        return {\n            \"Modelo\": name,\n            \"Accuracy (Val)\": val_acc,\n            \"F1-score (Val)\": val_f1,\n            \"Nº Parámetros\": params,\n            \"Tiempo (s)\": time_val\n        }\n    except Exception as e:\n        print(f\"Error al procesar métricas de '{name}': {e}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T21:07:18.470792Z","iopub.execute_input":"2025-12-30T21:07:18.471575Z","iopub.status.idle":"2025-12-30T21:07:18.478123Z","shell.execute_reply.started":"2025-12-30T21:07:18.471545Z","shell.execute_reply":"2025-12-30T21:07:18.477495Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Recopilando métricas...\")\n\nbest_model_path = 'best_transfer_.keras' \ntry:\n    best_model = tf.keras.models.load_model(\n        best_model_path,\n        custom_objects={'SparseF1Score': SparseF1Score} \n    )\n    print(\"Modelo cargado.\")\nexcept:\n    print(f\"Error cargando {best_model_path}.\")\n    best_model = None\n\nmetrics_data = []\n\nmetrics_data.append(save_metric('mlp_model', 'history_mlp', 'MLP Baseline', 'mlp_training_time'))\nmetrics_data.append(save_metric('cnn_model', 'history_cnn', 'CNN Propia', 'cnn_training_time'))\nfor name, hist in results.items():\n    metrics_data.append(save_metric(\n        model_input=models[name],     \n        history_input=hist,              \n        name=f\"TL: {name}\",              \n        time_input=times.get(name, 0) \n    ))\n    \nmetrics_data.append(save_metric('vit_model', 'history_vit', 'Vision Transformer', 'vit_training_time'))\n\n# Limpieza de RAM\nheavy_vars = ['mlp_model', 'cnn_model', 'transfer_model', 'base_model', 'vit_model',\n              'train_ds', 'val_ds', 'train_ds_vit', 'val_ds_vit', 'test_images', 'test_probs']\nfor var in heavy_vars:\n    if var in globals(): del globals()[var]\ntf.keras.backend.clear_session()\ngc.collect()\n\n\n# Filtramos 'allBands' si existe\nreal_class_names = [c for c in class_names if c != 'allBands']\nprint(f\"Clases válidas para evaluación ({len(real_class_names)}): {real_class_names}\")\n\nif best_model:\n    print(\"Generando predicciones...\")\n    y_true, y_pred, error_samples = [], [], []\n    \n    # Si test_ds fue borrado por el script anterior de limpieza, lo recargamos:\n    if 'test_ds' not in globals():\n        test_ds = tf.keras.utils.image_dataset_from_directory(\n            data_dir, validation_split=0.15, subset=\"validation\", seed=2025,\n            image_size=(IMG_HEIGHT, IMG_WIDTH), batch_size=32, label_mode='int'\n        )\n\n    for batch_images, batch_labels in test_ds:\n        preds = best_model.predict(batch_images, verbose=0)\n        preds_ids = np.argmax(preds, axis=1)\n        labels_ids = batch_labels.numpy().astype(int)\n        \n        valid_mask = labels_ids < 10 \n        \n        y_true.extend(labels_ids[valid_mask])\n        y_pred.extend(preds_ids[valid_mask])\n        \n        if len(error_samples) < 5:\n            batch_errors = np.where((preds_ids != labels_ids) & valid_mask)[0]\n            for idx in batch_errors:\n                if len(error_samples) >= 5: break\n                error_samples.append({\n                    'img': batch_images[idx].numpy().astype(\"uint8\"),\n                    'true': real_class_names[labels_ids[idx]],\n                    'pred': real_class_names[preds_ids[idx]],\n                    'conf': np.max(preds[idx])\n                })\n        del batch_images, batch_labels, preds\n        gc.collect()\n\n    print(\"\\nMATRIZ DE CONFUSIÓN\")\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(10, 8))\n    # Usamos real_class_names que tiene el tamaño correcto\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=real_class_names, yticklabels=real_class_names)\n    plt.ylabel('Real')\n    plt.xlabel('Predicción')\n    plt.show()\n    \n    print(\"\\nCLASSIFICATION REPORT\")\n    print(classification_report(y_true, y_pred, target_names=real_class_names))\n    \n    if error_samples:\n        print(f\"\\nVisualización de errores:\")\n        plt.figure(figsize=(15, 4))\n        for i, sample in enumerate(error_samples):\n            ax = plt.subplot(1, 5, i + 1)\n            plt.imshow(sample['img'])\n            plt.title(f\"R: {sample['true']}\\nP: {sample['pred']}\\n{sample['conf']:.1%}\", \n                      color='red', fontsize=9)\n            plt.axis(\"off\")\n        plt.show()\n\n\ndf_results = pd.DataFrame([m for m in metrics_data if m is not None])\nif not df_results.empty:\n    df_results = df_results.sort_values(by=\"Accuracy (Val)\", ascending=False)\n    display(df_results)\n    \n    plt.figure(figsize=(10, 5))\n    sns.barplot(data=df_results, x='Modelo', y='Accuracy (Val)', palette='viridis')\n    plt.title(\"Comparativa Final\")\n    plt.ylim(0, 1.05)\n    for i, v in enumerate(df_results['Accuracy (Val)']):\n        plt.text(i, v + 0.01, f\"{v:.1%}\", ha='center')\n    plt.show()\n\n    df_results = df_results.sort_values(by=\"F1-score (Val)\", ascending=False)\n    display(df_results)\n    \n    plt.figure(figsize=(10, 5))\n    sns.barplot(data=df_results, x='Modelo', y='F1-score (Val)', palette='viridis')\n    plt.title(\"Comparativa Final\")\n    plt.ylim(0, 1.05)\n    for i, v in enumerate(df_results['F1-score (Val)']):\n        plt.text(i, v + 0.01, f\"{v:.1%}\", ha='center')\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2025-12-30T21:07:20.928426Z","iopub.execute_input":"2025-12-30T21:07:20.928723Z","iopub.status.idle":"2025-12-30T21:07:22.407172Z","shell.execute_reply.started":"2025-12-30T21:07:20.928698Z","shell.execute_reply":"2025-12-30T21:07:22.406483Z"},"papermill":{"duration":118.013136,"end_time":"2025-12-13T20:48:01.484662","exception":false,"start_time":"2025-12-13T20:46:03.471526","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Análisis Cuantitativo\nEl modelo basado en Transfer Learning se posiciona como la mejor arquitectura. Se ve que supera ampliamente a las otras opciones tanto en accuracy como en F1-score, mostrando un equilibrio muy sólido entre precisión y exhaustividad. Esto confirma que partir de un modelo pre-entrenado permite obtener métricas de rendimiento muy altas, incluso con un conjunto de datos de entrenamiento relativamente pequeño, superando las limitaciones de aprender desde cero.\n\nCon respecto a la matriz de confusion vemos una clara diagonal que nos indica que el modelo predice de forma correcta la mayoria de imagenes. Las mayores confusiones vienen por las parejas de Forest/SeaLake, River/Highway y HerbadeusVegetation/PermanentCrop. El primero de ellos puede deberse a la falta de resolucion de la imagen. En una imagen pequeña si solo cogiesemos el color de la misma, podriamos tener bosques mas azulados o lagos un poco mas verdosos. Cabe señalar que algunas de las imagenes mal clasificadas, son tan dificiles para el modelo como para nosotros, pues con una imagen tan pequeña es dificil obtener el contexto completo.\n\n","metadata":{"papermill":{"duration":1.879825,"end_time":"2025-12-13T20:48:05.175255","exception":false,"start_time":"2025-12-13T20:48:03.295430","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Análisis Cualitativo\nAl observar las imágenes mal clasificadas, notamos que los fallos no son aleatorios, sino que ocurren en situaciones de ambigüedad visual (como hemos indicado antes, por basar parte de la información en los colores, una recta de color medio gris/azul que puede ser asfalto o agua...). En muchas de estas predicciones erróneas, el modelo asigna un nivel de confianza más bajo que en sus aciertos, lo que sugiere que la red es consciente de la simulitud entre ambas clases.","metadata":{"papermill":{"duration":1.911875,"end_time":"2025-12-13T20:48:12.792083","exception":false,"start_time":"2025-12-13T20:48:10.880208","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Comparativa final\nEl modelo MLP resultó ineficaz al perder la información espacial (aplanado de imagen), y nuestra CNN, aunque capaz de extraer características, se ha visto muy limitada. El Transfer Learning logró el equilibrio ideal: a pesar de tener una gran cantidad de parámetros totales, el uso de capas congeladas permitió un entrenamiento rápido y una capacidad de generalización muy superior. Por otro lado, el ViT consume muchos recursos pero no llega al nivel del Transfer Learning, por lo que no se justifica (al menos en este dataset) su uso. \n\nEs por eso que hemos elegido el Transfer Learning como mejor arquitectura, ya que ofrece la máxima precisión con un coste de entrenamiento razonable.\n\nComo nota final, en lo que respecta a la forma de entrenamiento, hemos descubierto que la técnica Early Stopping es muy util si los datos que vamos a predecir cambian. Si el dataset al que tiene que enfrentarse el modelo no cambiase (como en una competición) esto permite un mayor overfitting pero se traduce tambien en mejores resultados (hasta cierto punto). Esto confirma lo que hemos ido viendo en clase a lo largo del cuatrimestre.","metadata":{"papermill":{"duration":1.881643,"end_time":"2025-12-13T20:48:20.331328","exception":false,"start_time":"2025-12-13T20:48:18.449685","status":"completed"},"tags":[]}}]}
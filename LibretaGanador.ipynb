{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fb85da0",
   "metadata": {},
   "source": [
    "# Libreta para entrenar al modelo ganador\n",
    "\n",
    "Esta libreta servirá para entrenar al modelo ganador con el datasheet completo. Además, esta libreta incluye una serie de experimentos con el modelo ya entrenado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bacd8a",
   "metadata": {},
   "source": [
    "## Importar librerías y variables globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b20abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yt_dlp (from -r req.txt (line 1))\n",
      "  Using cached yt_dlp-2025.12.8-py3-none-any.whl.metadata (180 kB)\n",
      "Collecting tensorflow (from -r req.txt (line 2))\n",
      "  Using cached tensorflow-2.20.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting transformers (from -r req.txt (line 3))\n",
      "  Using cached transformers-4.57.6-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting tf-keras (from -r req.txt (line 4))\n",
      "  Using cached tf_keras-2.20.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting matplotlib (from -r req.txt (line 5))\n",
      "  Using cached matplotlib-3.10.8-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (52 kB)\n",
      "Collecting pandas (from -r req.txt (line 6))\n",
      "  Using cached pandas-2.3.3-cp313-cp313-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "Collecting seaborn (from -r req.txt (line 7))\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting scikit-learn (from -r req.txt (line 8))\n",
      "  Using cached scikit_learn-1.8.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow->-r req.txt (line 2))\n",
      "  Using cached absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow->-r req.txt (line 2))\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow->-r req.txt (line 2))\n",
      "  Using cached flatbuffers-25.12.19-py2.py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow->-r req.txt (line 2))\n",
      "  Using cached gast-0.7.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow->-r req.txt (line 2))\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow->-r req.txt (line 2))\n",
      "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow->-r req.txt (line 2))\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.13/site-packages (from tensorflow->-r req.txt (line 2)) (25.0)\n",
      "Collecting protobuf>=5.28.0 (from tensorflow->-r req.txt (line 2))\n",
      "  Using cached protobuf-6.33.4-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting requests<3,>=2.21.0 (from tensorflow->-r req.txt (line 2))\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting setuptools (from tensorflow->-r req.txt (line 2))\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.13/site-packages (from tensorflow->-r req.txt (line 2)) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow->-r req.txt (line 2))\n",
      "  Using cached termcolor-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting typing_extensions>=3.6.6 (from tensorflow->-r req.txt (line 2))\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow->-r req.txt (line 2))\n",
      "  Using cached wrapt-2.0.1-cp313-cp313-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow->-r req.txt (line 2))\n",
      "  Using cached grpcio-1.76.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow->-r req.txt (line 2))\n",
      "  Using cached tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.10.0 (from tensorflow->-r req.txt (line 2))\n",
      "  Using cached keras-3.13.1-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting numpy>=1.26.0 (from tensorflow->-r req.txt (line 2))\n",
      "  Using cached numpy-2.4.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow->-r req.txt (line 2))\n",
      "  Using cached h5py-3.15.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow->-r req.txt (line 2))\n",
      "  Using cached ml_dtypes-0.5.4-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests<3,>=2.21.0->tensorflow->-r req.txt (line 2))\n",
      "  Using cached charset_normalizer-3.4.4-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorflow->-r req.txt (line 2))\n",
      "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorflow->-r req.txt (line 2))\n",
      "  Using cached urllib3-2.6.3-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorflow->-r req.txt (line 2))\n",
      "  Using cached certifi-2026.1.4-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.20.0->tensorflow->-r req.txt (line 2))\n",
      "  Using cached markdown-3.10-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting pillow (from tensorboard~=2.20.0->tensorflow->-r req.txt (line 2))\n",
      "  Using cached pillow-12.1.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow->-r req.txt (line 2))\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow->-r req.txt (line 2))\n",
      "  Using cached werkzeug-3.1.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting filelock (from transformers->-r req.txt (line 3))\n",
      "  Using cached filelock-3.20.3-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers->-r req.txt (line 3))\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting pyyaml>=5.1 (from transformers->-r req.txt (line 3))\n",
      "  Using cached pyyaml-6.0.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers->-r req.txt (line 3))\n",
      "  Using cached regex-2026.1.15-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers->-r req.txt (line 3))\n",
      "  Using cached tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers->-r req.txt (line 3))\n",
      "  Using cached safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers->-r req.txt (line 3))\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.34.0->transformers->-r req.txt (line 3))\n",
      "  Using cached fsspec-2026.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers->-r req.txt (line 3))\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->-r req.txt (line 5))\n",
      "  Using cached contourpy-1.3.3-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->-r req.txt (line 5))\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->-r req.txt (line 5))\n",
      "  Using cached fonttools-4.61.1-cp313-cp313-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (114 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->-r req.txt (line 5))\n",
      "  Using cached kiwisolver-1.4.9-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
      "Collecting pyparsing>=3 (from matplotlib->-r req.txt (line 5))\n",
      "  Using cached pyparsing-3.3.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.13/site-packages (from matplotlib->-r req.txt (line 5)) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->-r req.txt (line 6))\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->-r req.txt (line 6))\n",
      "  Using cached tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting scipy>=1.10.0 (from scikit-learn->-r req.txt (line 8))\n",
      "  Using cached scipy-1.17.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting joblib>=1.3.0 (from scikit-learn->-r req.txt (line 8))\n",
      "  Using cached joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting threadpoolctl>=3.2.0 (from scikit-learn->-r req.txt (line 8))\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow->-r req.txt (line 2))\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rich (from keras>=3.10.0->tensorflow->-r req.txt (line 2))\n",
      "  Using cached rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.10.0->tensorflow->-r req.txt (line 2))\n",
      "  Using cached namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.10.0->tensorflow->-r req.txt (line 2))\n",
      "  Using cached optree-0.18.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (34 kB)\n",
      "Collecting markupsafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow->-r req.txt (line 2))\n",
      "  Using cached markupsafe-3.0.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.10.0->tensorflow->-r req.txt (line 2))\n",
      "  Using cached markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.13/site-packages (from rich->keras>=3.10.0->tensorflow->-r req.txt (line 2)) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow->-r req.txt (line 2))\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached yt_dlp-2025.12.8-py3-none-any.whl (3.3 MB)\n",
      "Using cached tensorflow-2.20.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.8 MB)\n",
      "Using cached grpcio-1.76.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (6.6 MB)\n",
      "Using cached ml_dtypes-0.5.4-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.0 MB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.4-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
      "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Downloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading urllib3-2.6.3-py3-none-any.whl (131 kB)\n",
      "Downloading transformers-4.57.6-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tf_keras-2.20.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.10.8-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.3-cp313-cp313-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading scikit_learn-1.8.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Downloading certifi-2026.1.4-py3-none-any.whl (152 kB)\n",
      "Downloading contourpy-1.3.3-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (362 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading flatbuffers-25.12.19-py2.py3-none-any.whl (26 kB)\n",
      "Downloading fonttools-4.61.1-cp313-cp313-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading fsspec-2026.1.0-py3-none-any.whl (201 kB)\n",
      "Downloading gast-0.7.0-py3-none-any.whl (22 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading h5py-3.15.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Downloading keras-3.13.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.9-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading markdown-3.10-py3-none-any.whl (107 kB)\n",
      "Downloading numpy-2.4.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading pillow-12.1.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.33.4-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading pyparsing-3.3.1-py3-none-any.whl (121 kB)\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading pyyaml-6.0.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (801 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.6/801.6 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2026.1.15-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.6/803.6 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "Downloading scipy-1.17.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (35.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.0/35.0 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Downloading termcolor-3.3.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
      "Downloading werkzeug-3.1.5-py3-none-any.whl (225 kB)\n",
      "Downloading markupsafe-3.0.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n",
      "Downloading wrapt-2.0.1-cp313-cp313-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (121 kB)\n",
      "Downloading filelock-3.20.3-py3-none-any.whl (16 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.18.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (414 kB)\n",
      "Downloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: pytz, namex, libclang, flatbuffers, yt_dlp, wrapt, wheel, urllib3, tzdata, typing_extensions, tqdm, threadpoolctl, termcolor, tensorboard-data-server, setuptools, safetensors, regex, pyyaml, pyparsing, protobuf, pillow, opt_einsum, numpy, mdurl, markupsafe, markdown, kiwisolver, joblib, idna, hf-xet, google_pasta, gast, fsspec, fonttools, filelock, cycler, charset_normalizer, certifi, absl-py, werkzeug, scipy, requests, pandas, optree, ml_dtypes, markdown-it-py, h5py, grpcio, contourpy, astunparse, tensorboard, scikit-learn, rich, matplotlib, huggingface-hub, tokenizers, seaborn, keras, transformers, tensorflow, tf-keras\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61/61\u001b[0m [tf-keras]tf-keras]tensorflow]s]ub]s]\n",
      "\u001b[1A\u001b[2KSuccessfully installed absl-py-2.3.1 astunparse-1.6.3 certifi-2026.1.4 charset_normalizer-3.4.4 contourpy-1.3.3 cycler-0.12.1 filelock-3.20.3 flatbuffers-25.12.19 fonttools-4.61.1 fsspec-2026.1.0 gast-0.7.0 google_pasta-0.2.0 grpcio-1.76.0 h5py-3.15.1 hf-xet-1.2.0 huggingface-hub-0.36.0 idna-3.11 joblib-1.5.3 keras-3.13.1 kiwisolver-1.4.9 libclang-18.1.1 markdown-3.10 markdown-it-py-4.0.0 markupsafe-3.0.3 matplotlib-3.10.8 mdurl-0.1.2 ml_dtypes-0.5.4 namex-0.1.0 numpy-2.4.1 opt_einsum-3.4.0 optree-0.18.0 pandas-2.3.3 pillow-12.1.0 protobuf-6.33.4 pyparsing-3.3.1 pytz-2025.2 pyyaml-6.0.3 regex-2026.1.15 requests-2.32.5 rich-14.2.0 safetensors-0.7.0 scikit-learn-1.8.0 scipy-1.17.0 seaborn-0.13.2 setuptools-80.9.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.3.0 tf-keras-2.20.1 threadpoolctl-3.6.0 tokenizers-0.22.2 tqdm-4.67.1 transformers-4.57.6 typing_extensions-4.15.0 tzdata-2025.3 urllib3-2.6.3 werkzeug-3.1.5 wheel-0.45.1 wrapt-2.0.1 yt_dlp-2025.12.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r req.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81871ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Collecting numpy<2.3.0,>=2 (from opencv-python)\n",
      "  Downloading numpy-2.2.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Using cached opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (67.0 MB)\n",
      "Downloading numpy-2.2.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, opencv-python\n",
      "\u001b[2K  Attempting uninstall: numpy\n",
      "\u001b[2K    Found existing installation: numpy 2.4.1\n",
      "\u001b[2K    Uninstalling numpy-2.4.1:\n",
      "\u001b[2K      Successfully uninstalled numpy-2.4.1\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [opencv-python]0m [opencv-python]\n",
      "\u001b[1A\u001b[2KSuccessfully installed numpy-2.2.6 opencv-python-4.12.0.88\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb06a5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Usando librería 'tf_keras' explícita (Modo Legacy seguro).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo/Documentos/reconocimiento-visual-de-videojuegos/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versión de TensorFlow: 2.20.0\n",
      "Backend de Keras en uso: tf_keras\n",
      "GPU Disponible: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-17 16:51:43.808073: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 1. Configuración de entorno (Debe ir ANTES de importar tensorflow)\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pathlib\n",
    "import PIL.Image\n",
    "import time\n",
    "import gc \n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from IPython.display import clear_output, display\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "# 2. CORRECCIÓN DE IMPORTS DE KERAS\n",
    "# Intentamos usar tf_keras explícitamente para asegurar compatibilidad Legacy\n",
    "try:\n",
    "    import tf_keras as keras\n",
    "    from tf_keras import layers, models, callbacks, mixed_precision\n",
    "    from tf_keras.models import load_model, Model\n",
    "    print(f\"✅ Usando librería 'tf_keras' explícita (Modo Legacy seguro).\")\n",
    "except ImportError:\n",
    "    # Si no está instalado tf_keras, forzamos la consistencia usando tf.keras\n",
    "    from tensorflow import keras\n",
    "    layers = keras.layers\n",
    "    models = keras.models\n",
    "    callbacks = keras.callbacks\n",
    "    mixed_precision = keras.mixed_precision\n",
    "    load_model = keras.models.load_model\n",
    "    Model = keras.models.Model\n",
    "    print(f\"⚠️ 'tf_keras' no encontrado. Usando 'tensorflow.keras' vinculado.\")\n",
    "\n",
    "# 3. Imports de Transformers (se adaptarán a la versión de Keras cargada por TF)\n",
    "from transformers import TFViTModel\n",
    "from transformers import TFAutoModel\n",
    "\n",
    "# Verificaciones\n",
    "print(\"Versión de TensorFlow:\", tf.__version__)\n",
    "print(\"Backend de Keras en uso:\", keras.__name__)\n",
    "print(f\"GPU Disponible: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51a197ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODO RÁPIDO ACTIVADO (FAST_EXEC = True)\n",
      "   Todas las épocas de entrenamiento se reducen\n",
      "Versión de TensorFlow: 2.20.0\n",
      "Tipo implementacion de Keras: {'tf_keras.api._v2.keras'}\n",
      "GPU Disponible: False\n"
     ]
    }
   ],
   "source": [
    "SEED = 2026\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "#mixed_precision.set_global_policy('mixed_float16') #baja uso de ram usando la mitad de precision en los float. \n",
    "\n",
    "FAST_EXEC = True\n",
    "GENERAL_EPOCHS = 20\n",
    "FAST_EPOCHS = 1\n",
    "if FAST_EXEC:\n",
    "    epochs_to_use = FAST_EPOCHS\n",
    "else:\n",
    "    epochs_to_use = GENERAL_EPOCHS\n",
    "\n",
    "if FAST_EXEC:\n",
    "    print(f\"MODO RÁPIDO ACTIVADO (FAST_EXEC = True)\")\n",
    "    print(f\"   Todas las épocas de entrenamiento se reducen\")\n",
    "\n",
    "print(\"Versión de TensorFlow:\",tf.__version__)\n",
    "print(\"Tipo implementacion de Keras:\",{tf.keras.__name__})\n",
    "print(f\"GPU Disponible: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a7c719",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5431512a",
   "metadata": {},
   "source": [
    "### Cargar y explorar dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "150281d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejecución en un entorno local. Ruta al dataset: ./images_dataset\n",
      "\n",
      "Resumen de TFRecords encontrados:\n",
      " ├─ Train shards: 0\n",
      " ├─ Val shards:   0\n",
      " └─ Test shards:  0\n",
      "Total archivos .tfrec: 0\n",
      "No se han encontrado archivos .tfrec\n"
     ]
    }
   ],
   "source": [
    "if FAST_EXEC:\n",
    "    KAGGLE_PATH = '/kaggle/input/videojuegos-small-tfrec/tfrecords_small_dataset'\n",
    "else:\n",
    "    KAGGLE_PATH = '/kaggle/input/videojuegos-tfrec/tfrecords_dataset'\n",
    "# Ruta que tendria en local.\n",
    "LOCAL_PATH = './images_dataset' \n",
    "\n",
    "# Aunque las imágenes son mayores, el primer modelo (MLP) funciona mejor con 64x64\n",
    "BATCH_SIZE = 64\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "\"\"\"BATCH_SIZE = 16\n",
    "IMG_HEIGHT = 480\n",
    "IMG_WIDTH = 854\"\"\"\n",
    "\n",
    "# Para saber si es Kaggle y cambiar la ruta del dataset buscamos 'KAGGLE_KERNEL_RUN_TYPE' en el entorno\n",
    "if os.environ.get('KAGGLE_KERNEL_RUN_TYPE'):\n",
    "    data_dir = KAGGLE_PATH\n",
    "    print(\"Ejecución en Kaggle detectada. Ruta al dataset:\",data_dir)\n",
    "else:\n",
    "    data_dir = LOCAL_PATH\n",
    "    print(\"Ejecución en un entorno local. Ruta al dataset:\",data_dir)\n",
    "\n",
    "data_dir = pathlib.Path(data_dir)\n",
    "\n",
    "# Verificar estructura y contar archivos tfrec\n",
    "train_shards = list(data_dir.glob('train/*.tfrec'))\n",
    "val_shards = list(data_dir.glob('val/*.tfrec'))\n",
    "test_shards = list(data_dir.glob('test/*.tfrec'))\n",
    "\n",
    "total_shards = len(train_shards) + len(val_shards) + len(test_shards)\n",
    "\n",
    "print(f\"\\nResumen de TFRecords encontrados:\")\n",
    "print(f\" ├─ Train shards: {len(train_shards)}\")\n",
    "print(f\" ├─ Val shards:   {len(val_shards)}\")\n",
    "print(f\" └─ Test shards:  {len(test_shards)}\")\n",
    "print(f\"Total archivos .tfrec: {total_shards}\")\n",
    "\n",
    "# Cargar una imagen para ver si se lee bien\n",
    "if total_shards == 0:\n",
    "    print(\"No se han encontrado archivos .tfrec\")\n",
    "else:\n",
    "    # Cogemos el primer archivo que encontremos\n",
    "    sample_file = str(train_shards[0])\n",
    "    \n",
    "    print(f\"\\nInspeccionando primer archivo: {os.path.basename(sample_file)}...\")\n",
    "    \n",
    "    # Leemos un solo registro\n",
    "    raw_dataset = tf.data.TFRecordDataset(sample_file)\n",
    "    for raw_record in raw_dataset.take(1):\n",
    "        # Parseamos manualmente para ver qué hay dentro\n",
    "        example = tf.train.Example()\n",
    "        example.ParseFromString(raw_record.numpy())\n",
    "        \n",
    "        # Extraer etiqueta\n",
    "        label = example.features.feature['label'].int64_list.value[0]\n",
    "        \n",
    "        # Extraer imagen y decodificar\n",
    "        img_raw = example.features.feature['image'].bytes_list.value[0]\n",
    "        img_tensor = tf.io.decode_jpeg(img_raw)\n",
    "        \n",
    "        print(f\" Lectura exitosa.\")\n",
    "        print(f\" - Etiqueta (int): {label}\")\n",
    "        print(f\" - Shape original guardado: {img_tensor.shape}\")\n",
    "        print(f\" - Tipo de dato: {img_tensor.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe3bab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deben estar en orden alfabético estricto, igual que como se crearon los TFRecords\n",
    "if FAST_EXEC:\n",
    "    class_names = [\n",
    "        'AMONG_US', 'CONTRA', 'ELDEN_RING', 'GOD_OF_WAR_1', 'GTA_SAN_ANDREAS', 'GTA_V', 'HADES', \n",
    "        'HOLLOW_KNIGHT', 'MARIO_GALAXY', 'MARIO_KART_8', 'MINECRAFT', 'NEW_SUPER_MARIO_BROS', \n",
    "        'POKEMON_X_Y', 'RED_DEAD_REDEMPTION_2', 'SILENT_HILL_2', 'UNDERTALE'\n",
    "    ]\n",
    "else:\n",
    "    class_names = [\n",
    "        'AMONG_US', 'CONTRA', 'DEATH_STRANDING', 'ELDEN_RING', 'GOD_OF_WAR_1', \n",
    "        'GOD_OF_WAR_2018', 'GTA_SAN_ANDREAS', 'GTA_V', 'HADES', 'HOLLOW_KNIGHT', \n",
    "        'HORIZON_FORBBIDEN_WEST', 'HORIZON_ZERO_DAWN', 'MARIO_GALAXY', 'MARIO_KART_8', \n",
    "        'MARIO_KART_WORLD', 'MARIO_ODYSSEY', 'MINECRAFT', 'NEW_SUPER_MARIO_BROS', \n",
    "        'POKEMON_EMERALD', 'POKEMON_RED', 'POKEMON_X_Y', 'PUNCH_OUT_NES', \n",
    "        'RED_DEAD_REDEMPTION_1', 'RED_DEAD_REDEMPTION_2', 'SILENT_HILL_2', \n",
    "        'SILKSONG', 'STRAY', 'SUPER_MARIO_64', 'SUPER_MARIO_BROS', \n",
    "        'SUPER_MARIO_BROS_WONDER', 'UNDERTALE'\n",
    "    ]\n",
    "\n",
    "# Lectura \n",
    "def parse_tfrecord_fn(example, target_size):\n",
    "    feature_description = {\n",
    "        'image': tf.io.FixedLenFeature([], tf.string),\n",
    "        'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    image = tf.io.decode_jpeg(example['image'], channels=3)\n",
    "    image = tf.image.resize(image, target_size) # Redimensión dinámica AQUÍ\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    label = example['label']\n",
    "    return image, label\n",
    "\n",
    "def get_dataset_from_tfrecords(tfrecords_dir, batch_size=64, target_size=(224, 224)):\n",
    "    filenames = tf.io.gfile.glob(f\"{tfrecords_dir}/*.tfrec\")\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=tf.data.AUTOTUNE)\n",
    "    \n",
    "    ignore_order = tf.data.Options()\n",
    "    ignore_order.experimental_deterministic = False\n",
    "    dataset = dataset.with_options(ignore_order)\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        lambda x: parse_tfrecord_fn(x, target_size), \n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    dataset = dataset.shuffle(2000)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9aebca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando Batch Size REDUCIDO: 16\n"
     ]
    }
   ],
   "source": [
    "USE_REDUCED_BATCH_SIZE = True \n",
    "\n",
    "if USE_REDUCED_BATCH_SIZE:\n",
    "    BATCH_SIZE_VIT = 16\n",
    "    print(f\"Usando Batch Size REDUCIDO: {BATCH_SIZE_VIT}\")\n",
    "else:\n",
    "    BATCH_SIZE_VIT = 64\n",
    "    print(f\"Usando Batch Size: {BATCH_SIZE_VIT}\")\n",
    "\n",
    "train_ds_vit = get_dataset_from_tfrecords(os.path.join(data_dir, 'train'), batch_size=BATCH_SIZE_VIT, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "val_ds_vit = get_dataset_from_tfrecords(os.path.join(data_dir, 'val'), batch_size=BATCH_SIZE_VIT, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "test_ds_vit = get_dataset_from_tfrecords(os.path.join(data_dir, 'test'), batch_size=BATCH_SIZE_VIT, target_size=(IMG_HEIGHT, IMG_WIDTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b18e1b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculando pesos de las clases \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-17 16:53:03.703817: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPesos calculados exitosamente.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m class_weight_dict\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m class_weights = \u001b[43mcalculate_class_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds_vit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(class_weights)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mcalculate_class_weights\u001b[39m\u001b[34m(train_ds)\u001b[39m\n\u001b[32m     16\u001b[39m y_train = np.array(y_train)\n\u001b[32m     17\u001b[39m classes = np.unique(y_train)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m weights = \u001b[43mclass_weight\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_class_weight\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbalanced\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_train\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m class_weight_dict = \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(classes, weights))\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPesos calculados exitosamente.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/reconocimiento-visual-de-videojuegos/.venv/lib/python3.13/site-packages/sklearn/utils/_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/reconocimiento-visual-de-videojuegos/.venv/lib/python3.13/site-packages/sklearn/utils/class_weight.py:84\u001b[39m, in \u001b[36mcompute_class_weight\u001b[39m\u001b[34m(class_weight, classes, y, sample_weight)\u001b[39m\n\u001b[32m     80\u001b[39m     weighted_class_counts = np.bincount(y_ind, weights=sample_weight)\n\u001b[32m     81\u001b[39m     recip_freq = weighted_class_counts.sum() / (\n\u001b[32m     82\u001b[39m         \u001b[38;5;28mlen\u001b[39m(le.classes_) * weighted_class_counts\n\u001b[32m     83\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     weight = \u001b[43mrecip_freq\u001b[49m\u001b[43m[\u001b[49m\u001b[43mle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     86\u001b[39m     \u001b[38;5;66;03m# user-defined dictionary\u001b[39;00m\n\u001b[32m     87\u001b[39m     weight = np.ones(classes.shape[\u001b[32m0\u001b[39m], dtype=np.float64, order=\u001b[33m\"\u001b[39m\u001b[33mC\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mIndexError\u001b[39m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "def calculate_class_weights(train_ds):\n",
    "    \"\"\"\n",
    "    Extrae las etiquetas del dataset de entrenamiento y calcula los pesos\n",
    "    para equilibrar las clases durante el entrenamiento.\n",
    "    \"\"\"\n",
    "    print(\"Calculando pesos de las clases \")\n",
    "    \n",
    "    # Mapeamos el dataset para que solo devuelva las etiquetas y. Evita decodificar las imágenes pesadas\n",
    "    train_labels_only = train_ds.map(lambda x, y: y)\n",
    "    \n",
    "    # Ahora iteramos sobre un dataset de solo enteros (muy ligero)\n",
    "    y_train = []\n",
    "    for label_batch in train_labels_only:\n",
    "        y_train.extend(label_batch.numpy())\n",
    "        \n",
    "    y_train = np.array(y_train)\n",
    "    classes = np.unique(y_train)\n",
    "    \n",
    "    weights = class_weight.compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=classes,\n",
    "        y=y_train\n",
    "    )\n",
    "    \n",
    "    class_weight_dict = dict(zip(classes, weights))\n",
    "    \n",
    "    print(\"Pesos calculados exitosamente.\")\n",
    "    return class_weight_dict\n",
    "\n",
    "class_weights = calculate_class_weights(train_ds_vit)\n",
    "\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278632aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función auxiliar para cargar TODO el dataset junto para visualizarlo entero\n",
    "def get_full_dataset_for_eda(tfrecords_dir, batch_size=64, target_size=(64, 64)):\n",
    "    # \"*/*.tfrec\" busca dentro de train, val y test a la vez\n",
    "    filenames = tf.io.gfile.glob(f\"{tfrecords_dir}/*/*.tfrec\")\n",
    "    \n",
    "    print(f\"Cargando full_ds desde {len(filenames)} archivos TFRecord\")\n",
    "    \n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=tf.data.AUTOTUNE)\n",
    "    \n",
    "    dataset = dataset.map(\n",
    "        lambda x: parse_tfrecord_fn(x, target_size), \n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Generamos la variable full_ds\n",
    "full_ds = get_full_dataset_for_eda(data_dir, batch_size=BATCH_SIZE, target_size=(IMG_HEIGHT, IMG_WIDTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8cf90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 1. Usamos 'unbatch()' para sacar las imágenes de los paquetes\n",
    "# 2. Usamos 'take(25)' para coger exactamente las que necesitamos\n",
    "# 3. Usamos enumerate para saber en qué posición (i) del subplot estamos\n",
    "for i, (image, label) in enumerate(train_ds_vit.unbatch().take(25)):\n",
    "    \n",
    "    ax = plt.subplot(5, 5, i + 1)\n",
    "    \n",
    "    # Ya no necesitamos [i] porque 'image' es una sola foto, no un lote\n",
    "    plt.imshow(image.numpy().astype(\"uint8\"))\n",
    "    \n",
    "    # Manejo del label\n",
    "    label_index = int(label) \n",
    "    plt.title(class_names[label_index])\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Imagenes de ejemplo del dataset\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "class_counts = {name: 0 for name in class_names}\n",
    "for _, labels in full_ds:\n",
    "    for label in labels:\n",
    "        class_name = class_names[int(label)]\n",
    "        class_counts[class_name] += 1\n",
    "\n",
    "# Lo hacemos con un dataframe, pues es mas facil hacer el plot.\n",
    "df_counts = pd.DataFrame(list(class_counts.items()), columns=['Class', 'Count'])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "bplot = sns.barplot(x='Count', y='Class', data=df_counts, palette='viridis', hue='Class')\n",
    "# tenemos que quitar la leyenda manualmente porque da error con legend=False\n",
    "if bplot.get_legend() is not None:\n",
    "    bplot.get_legend().remove()\n",
    "\n",
    "plt.title('Distribucion de imagenes en el dataset', fontsize=16)\n",
    "plt.xlabel('Numero de imagenes', fontsize=12)\n",
    "plt.ylabel('Clase', fontsize=12)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "# añadimos el numero al final de la barra para verlo mejor\n",
    "for index, value in enumerate(df_counts['Count']):\n",
    "    plt.text(value + 50, index, str(value), va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "mean_count = df_counts['Count'].mean()\n",
    "std_count = df_counts['Count'].std()\n",
    "print(f\"Media de imagenes: {mean_count:.2f}\")\n",
    "print(f\"Desviacion estandar: {std_count:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1240d7c4",
   "metadata": {},
   "source": [
    "### Clases y funciones para el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95fcc468",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFVisionWrapper(layers.Layer):\n",
    "    def __init__(self, model_name, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.model_name = model_name\n",
    "        # TFAutoModel carga la arquitectura del modelo\n",
    "        self.hf_model = TFAutoModel.from_pretrained(model_name, from_pt=True)\n",
    "        self.hf_model.trainable = False # Congelamos los pesos base\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        outputs = self.hf_model(pixel_values=inputs)\n",
    "        \n",
    "        # ESTRATEGIA DE SALIDA:\n",
    "        # Si el modelo tiene 'pooler_output' (ej. ViT, DeiT), usamos eso (es el token CLS procesado).\n",
    "        if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
    "            return outputs.pooler_output\n",
    "        \n",
    "        # Si no (ej. Swin), tomamos el 'last_hidden_state' y hacemos un promedio.\n",
    "        else:\n",
    "            return tf.reduce_mean(outputs.last_hidden_state, axis=1)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"model_name\": self.model_name})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7eca1798",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseF1Score(tf.keras.metrics.F1Score):\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "       \n",
    "        num_classes = tf.shape(y_pred)[-1]  # numero de clases de la predicción\n",
    "        y_true = tf.reshape(y_true, [-1])\n",
    "        y_true_one_hot = tf.one_hot(tf.cast(y_true, tf.int32), num_classes) # Convertimos los enteros (sparse) a One-Hot\n",
    "        \n",
    "        super().update_state(y_true_one_hot, y_pred, sample_weight) # Llamamos a la función original con los datos corregidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1240a900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hf_classifier(model_name, img_height, img_width, num_classes=31, preprocessing='neg_one_to_one'):\n",
    "    inputs = keras.Input(shape=(img_height, img_width, 3))\n",
    "\n",
    "    # hacemos el data augmentation aquí para solucionar errores con Swin\n",
    "    data_augmentation = keras.Sequential([\n",
    "        layers.RandomFlip(\"horizontal_and_vertical\"), \n",
    "        layers.RandomRotation(0.05), # menor rotacion porque los Transformer son mas sensibles a esta\n",
    "        # Con suerte esto cambia los objetos entre distintos parches\n",
    "        layers.RandomTranslation(0.1, 0.1), \n",
    "        layers.RandomZoom(0.1), \n",
    "        layers.RandomContrast(0.2),\n",
    "        layers.RandomBrightness(0.1)\n",
    "    ], name=\"transformer_augmentation\")\n",
    "    # Podriamos añadir un randomCutout, para quitar partes aleatorias de la imagen (si da tiempo)\n",
    "    \n",
    "    x = data_augmentation(inputs)\n",
    "    \n",
    "    x = layers.Resizing(224, 224)(x)\n",
    "    \n",
    "    if preprocessing == 'neg_one_to_one':\n",
    "        x = layers.Rescaling(1./127.5, offset=-1)(x)\n",
    "    elif preprocessing == 'zero_to_one':\n",
    "        x = layers.Rescaling(1./255)(x)\n",
    "\n",
    "    # Reshape explícito para arreglar dimensiones perdidas por DataAugmentation, evita un error\n",
    "    x = layers.Reshape((224, 224, 3))(x)\n",
    "\n",
    "    # Pasamos a NCHW, que es el formato que usan\n",
    "    x = layers.Permute((3, 1, 2))(x)\n",
    "        \n",
    "    x = HFVisionWrapper(model_name)(x)\n",
    "    \n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    return keras.Model(inputs, outputs, name=f\"HF_{model_name.split('/')[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a68c2a2",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d24d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de modelos HF\n",
    "# (Nombre, Tipo de preprocesamiento recomendado)\n",
    "hf_candidates = [\n",
    "    # ViT\n",
    "    ('google/vit-base-patch16-224', 'neg_one_to_one', 2, 5), \n",
    "\n",
    "    # Apple MobileViT (Híbrido - CNN + Transformer Ligero)\n",
    "    #('apple/mobilevit-small', 'zero_to_one', 3, 7),\n",
    "    \n",
    "    # Facebook DeiT (Optimizado para entrenar con menos datos)\n",
    "    #('facebook/deit-tiny-patch16-224', 'zero_to_one', 4, 10) # tiny,small,base\n",
    "]\n",
    "\n",
    "# modelos descartados:\n",
    "    # Microsoft Swin (Jerárquico, suele batir a ViT). Usamos versión Tiny para ir rápido.\n",
    "    #('microsoft/swin-tiny-patch4-window7-224', 'zero_to_one'), da muchos problemas con el formato de la entrada, me rindo\n",
    "\n",
    "    # Facebook ConvNeXt (Versión Tiny)\n",
    "    #('facebook/convnext-tiny-224', 'zero_to_one'), ya lo tenemos en transfer learning\n",
    "\n",
    "if FAST_EXEC:\n",
    "    # Reconstruimos la lista de HF forzando 1 época en warmup y fine-tuning\n",
    "    # Mantenemos name y prep (índices 0 y 1) y cambiamos lo demás\n",
    "    hf_candidates = [\n",
    "        (name, prep, 1, 1) \n",
    "        for name, prep, _, _ in hf_candidates\n",
    "    ]\n",
    "\n",
    "results_hf = {}\n",
    "times_hf = {}\n",
    "\n",
    "print(f\"Comparativa de Transformers (Batch: {BATCH_SIZE_VIT}) ---\")\n",
    "\n",
    "for model_name, prep_type, e_warmup, e_fine in hf_candidates:\n",
    "    print(\"\\n>>> Entrenando: \",{model_name})\n",
    "\n",
    "    # Limpiamos sesiones anteriores para liberar VRAM de la GPU (en nuestros portátiles es necesario)\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Construimos el modelo\n",
    "    hf_model = build_hf_classifier(\n",
    "        model_name, \n",
    "        IMG_HEIGHT, \n",
    "        IMG_WIDTH, \n",
    "        num_classes=len(class_names), \n",
    "        preprocessing=prep_type\n",
    "    )\n",
    "    \n",
    "    # Compilamos\n",
    "    hf_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy', SparseF1Score(average='weighted', name='f1_score')]\n",
    "    )\n",
    "    \n",
    "    # Checkpoint específico para cada modelo\n",
    "    ckpt_name = f\"best_hf_{model_name.split('/')[-1]}.keras\"\n",
    "    callbacks_hf = [\n",
    "        callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1),\n",
    "        callbacks.ModelCheckpoint(ckpt_name, monitor='val_accuracy', save_best_only=True, verbose=0)\n",
    "    ]\n",
    "    \n",
    "    # Entrenar\n",
    "    start_time = time.time()\n",
    "    history_warmup = hf_model.fit(\n",
    "        train_ds_vit, \n",
    "        epochs=e_warmup,      \n",
    "        validation_data=val_ds_vit,\n",
    "        callbacks=callbacks_hf,\n",
    "        verbose=1,\n",
    "        class_weight=class_weights\n",
    "    )\n",
    "\n",
    "    print(\"    Descongelando para Fine Tuning\")\n",
    "    # Accedemos a la capa wrapper y descongelamos el modelo interno    \n",
    "    for layer in hf_model.layers:\n",
    "        if isinstance(layer, HFVisionWrapper): # Buscamos la capa wrapper por tipo o nombre\n",
    "            layer.hf_model.trainable = True   \n",
    "\n",
    "    hf_model.compile(\n",
    "        optimizer=keras.optimizers.Adam(1e-5), # learning rate MUYY bajo para evitar la perdida de los pesos pre entrenados\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy', SparseF1Score(average='weighted', name='f1_score')]\n",
    "    )\n",
    "\n",
    "    history_fine = hf_model.fit(\n",
    "        train_ds_vit, \n",
    "        epochs=e_fine,\n",
    "        validation_data=val_ds_vit,\n",
    "        callbacks=callbacks_hf,\n",
    "        verbose=1,\n",
    "        class_weight=class_weights\n",
    "    )\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Guardar métricas y limpiar\n",
    "    combined_history = {}\n",
    "    for key in history_warmup.history.keys():\n",
    "        list_1 = history_warmup.history.get(key, [])\n",
    "        list_2 = history_fine.history.get(key, [])\n",
    "        combined_history[key] = list_1 + list_2\n",
    "        \n",
    "    results_hf[model_name] = combined_history \n",
    "    times_hf[model_name] = total_time\n",
    "    \n",
    "    del hf_model  # Borramos la variable de Python\n",
    "    keras.backend.clear_session() # Borramos el grafo de TensorFlow\n",
    "    gc.collect()  # Forzamos al recolector de basura de Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5441337c",
   "metadata": {},
   "source": [
    "## Experimentos con el modelo entrenado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86b748a",
   "metadata": {},
   "source": [
    "### Configuración de variables globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eaed9351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo desde pruebasViT/best_hf_vit-base-patch16-224.keras...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n",
      "Loading a PyTorch model in TensorFlow, requires both PyTorch and TensorFlow to be installed. Please see https://pytorch.org/ and https://www.tensorflow.org/install/ for installation instructions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error al cargar el modelo: Error when deserializing class 'HFVisionWrapper' using config={'name': 'hf_vision_wrapper', 'trainable': True, 'dtype': 'float32', 'model_name': 'google/vit-base-patch16-224'}.\n",
      "\n",
      "Exception encountered: No module named 'torch'\n"
     ]
    }
   ],
   "source": [
    "# Configurar experimentos\n",
    "MODEL_PATH = 'pruebasViT/best_hf_vit-base-patch16-224.keras'\n",
    "TEST_IMAGES_DIR = 'pruebasViT/in-the-wild'  # Carpeta con imágenes in-the-wild\n",
    "TEST2_IMAGES_DIR = 'pruebasViT/capasIntermedias' # Carpeta con varias imágenes para ver las capas intermedias\n",
    "TEST_SINGLE_IMG_DIR = 'pruebasViT/robustez' # Carpeta con una imagen que predice correctamente por clase para el test de robustez\n",
    "VIDEO_PATH = 'pruebasViT/video_demo.mp4' # Vídeo con un gameplay para ver la predicción en tiempo real\n",
    "TARGET_SIZE = (224, 224)\n",
    "\n",
    "# Niveles de intensidad para los experimentos\n",
    "# Ruido (Sigma para distribución normal en imágenes normalizadas 0-1)\n",
    "NOISE_LEVELS = [0.00, 0.05, 0.10, 0.20] \n",
    "# Desenfoque (Tamaño del Kernel)\n",
    "BLUR_LEVELS = [1, 5, 11, 19]\n",
    "\n",
    "\n",
    "# Cargar el modelo ganador\n",
    "\n",
    "# Verificamos si el archivo existe antes de intentar cargarlo\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    try:\n",
    "        print(f\"Cargando modelo desde {MODEL_PATH}...\")\n",
    "        \n",
    "        # Es fundamental incluir 'SparseF1Score' en custom_objects porque el modelo\n",
    "        # se compiló usando esta métrica personalizada.\n",
    "        final_model = load_model(MODEL_PATH, custom_objects={'HFVisionWrapper': HFVisionWrapper, 'SparseF1Score': SparseF1Score}, compile=False)\n",
    "        \n",
    "        print(\"Modelo ganador cargado correctamente\")\n",
    "        print(\"\\n--- Resumen del Modelo ---\")\n",
    "        final_model.summary()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar el modelo: {e}\")\n",
    "else:\n",
    "    print(f\"No se encontró el archivo en: {MODEL_PATH}\")\n",
    "    print(\"Por favor, sube el archivo del modelo (.keras) y actualiza la variable MODEL_PATH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cb831f",
   "metadata": {},
   "source": [
    "### Definición de funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d4b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba con imágenes in-the-wild\n",
    "def visualize_wild_predictions(model, img_dir, classes, target_size=(224, 224)):\n",
    "    correct_predictions = []\n",
    "    incorrect_predictions = []\n",
    "\n",
    "    # Verificar si el directorio existe\n",
    "    if not os.path.exists(img_dir):\n",
    "        print(f\"⚠️ El directorio {img_dir} no existe.\")\n",
    "        return\n",
    "\n",
    "    # Recorremos las subcarpetas (que son las clases reales)\n",
    "    for class_folder in os.listdir(img_dir):\n",
    "        class_path = os.path.join(img_dir, class_folder)\n",
    "        \n",
    "        # Solo procesar si es un directorio\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "            \n",
    "        true_label = class_folder # La carpeta nos da la etiqueta real\n",
    "        \n",
    "        # Verificar si la carpeta corresponde a una clase conocida por el modelo\n",
    "        if true_label not in classes:\n",
    "            print(f\"⚠️ La carpeta '{true_label}' no está en la lista de clases del modelo. Se omitirá.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"📂 Procesando carpeta: {true_label}...\")\n",
    "        \n",
    "        files = [f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.webp'))]\n",
    "        \n",
    "        for file in files:\n",
    "            img_path = os.path.join(class_path, file)\n",
    "            \n",
    "            try:\n",
    "                # Cargar y preprocesar imagen\n",
    "                img = tf.keras.preprocessing.image.load_img(img_path, target_size=target_size)\n",
    "                img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "                img_array = img_array / 255.0  # Normalización\n",
    "                img_batch = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "                # Predicción\n",
    "                preds = model.predict(img_batch, verbose=0)\n",
    "                pred_idx = np.argmax(preds)\n",
    "                pred_label = classes[pred_idx]\n",
    "                confidence = np.max(preds) * 100\n",
    "\n",
    "                result_data = {\n",
    "                    'filename': file,\n",
    "                    'img': img,\n",
    "                    'true': true_label,\n",
    "                    'pred': pred_label,\n",
    "                    'conf': confidence\n",
    "                }\n",
    "\n",
    "                if pred_label == true_label:\n",
    "                    correct_predictions.append(result_data)\n",
    "                else:\n",
    "                    incorrect_predictions.append(result_data)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error procesando {file}: {e}\")\n",
    "\n",
    "    # --- FUNCIÓN AUXILIAR PARA PINTAR ---\n",
    "    def plot_grid(data_list, title, color_theme):\n",
    "        if not data_list:\n",
    "            print(f\"No hay imágenes para mostrar en: {title}\")\n",
    "            return\n",
    "            \n",
    "        n = len(data_list)\n",
    "        cols = 5\n",
    "        rows = (n // cols) + 1 if n % cols != 0 else n // cols\n",
    "        \n",
    "        plt.figure(figsize=(15, 3.5 * rows))\n",
    "        plt.suptitle(title, fontsize=16, weight='bold', color=color_theme, y=1.02)\n",
    "        \n",
    "        for i, item in enumerate(data_list):\n",
    "            ax = plt.subplot(rows, cols, i + 1)\n",
    "            ax.imshow(item['img'])\n",
    "            ax.axis('off')\n",
    "            \n",
    "            # Formato del título de cada imagen\n",
    "            if title.startswith(\"FALLOS\"):\n",
    "                info_text = f\"Pred: {item['pred']}\\nConf: {item['conf']:.1f}%\\nReal: {item['true']}\"\n",
    "                txt_color = 'red'\n",
    "            else:\n",
    "                info_text = f\"{item['pred']}\\n{item['conf']:.1f}%\"\n",
    "                txt_color = 'green'\n",
    "                \n",
    "            ax.set_title(info_text, color=txt_color, fontsize=10)\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Mostrar resultados\n",
    "    print(f\"\\nResultados: {len(correct_predictions)} aciertos, {len(incorrect_predictions)} fallos.\")\n",
    "    \n",
    "    plot_grid(correct_predictions, \"ACIERTOS (Predicciones Correctas)\", 'green')\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    plot_grid(incorrect_predictions, \"FALLOS (Confusiones)\", 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfe8159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver predicciones en tiempo real de un vídeo\n",
    "def process_and_display_video(video_path, model, classes, target_size):\n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"⚠️ No se encontró el video: {video_path}\")\n",
    "        return\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    def predict_frame(model, frame, classes, target_size):\n",
    "        \"\"\"\n",
    "        Función auxiliar tipo predict_top_k adaptada para un solo frame.\n",
    "        Devuelve la etiqueta, la confianza y las top 3 probabilidades.\n",
    "        \"\"\"\n",
    "        # Preprocesamiento\n",
    "        # 1. Resize\n",
    "        img_resized = cv2.resize(frame, target_size)\n",
    "        # 2. Convertir a array y expandir dimensiones\n",
    "        img_array = np.array(img_resized, dtype=\"float32\")\n",
    "        # 3. Normalizar (Asumiendo que entrenaste con rescale 1./255)\n",
    "        img_array = img_array / 255.0\n",
    "        img_batch = np.expand_dims(img_array, axis=0)\n",
    "        \n",
    "        # Predicción\n",
    "        preds = model.predict(img_batch, verbose=0)[0]\n",
    "        \n",
    "        # Obtener Top K (Top 3 para visualización)\n",
    "        top_k_indices = preds.argsort()[-3:][::-1]\n",
    "        top_predictions = [(classes[i], preds[i] * 100) for i in top_k_indices]\n",
    "        \n",
    "        return top_predictions\n",
    "    \n",
    "    try:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break # Fin del video\n",
    "\n",
    "            # --- LÓGICA DE PREDICCIÓN ---\n",
    "            # Para mejorar el rendimiento, puedes predecir cada N frames\n",
    "            # Aquí lo hacemos en todos para máxima fluidez visual\n",
    "            \n",
    "            # OpenCV usa BGR, Keras suele esperar RGB. Convertimos para la predicción.\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            top_preds = predict_frame(model, frame_rgb, classes, target_size)\n",
    "            \n",
    "            winner_label, winner_conf = top_preds[0]\n",
    "\n",
    "            # --- VISUALIZACIÓN EN EL VIDEO ---\n",
    "            # Dibujar un rectángulo de fondo para el texto\n",
    "            overlay = frame.copy()\n",
    "            cv2.rectangle(overlay, (0, 0), (300, 120), (0, 0, 0), -1)\n",
    "            alpha = 0.6 # Transparencia del fondo\n",
    "            frame = cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0)\n",
    "\n",
    "            # Escribir la predicción ganadora\n",
    "            color = (0, 255, 0) if winner_conf > 60 else (0, 165, 255) # Verde si seguro, Naranja si duda\n",
    "            \n",
    "            # Título principal\n",
    "            cv2.putText(frame, f\"{winner_label}\", (10, 40), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2, cv2.LINE_AA)\n",
    "            cv2.putText(frame, f\"Conf: {winner_conf:.1f}%\", (10, 70), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "            # Mostrar la 2da opción pequeña abajo (para ver si duda)\n",
    "            sec_label, sec_conf = top_preds[1]\n",
    "            cv2.putText(frame, f\"Alt: {sec_label} ({sec_conf:.1f}%)\", (10, 100), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1, cv2.LINE_AA)\n",
    "\n",
    "            # --- MOSTRAR EN JUPYTER ---\n",
    "            # Convertir BGR a RGB para mostrar correctamente con PIL\n",
    "            frame_display = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            img_display = PILImage.fromarray(frame_display)\n",
    "            \n",
    "            # Truco para actualizar la celda: borrar salida anterior y mostrar nueva\n",
    "            clear_output(wait=True)\n",
    "            display(img_display)\n",
    "            \n",
    "            # Control de velocidad (opcional, quitar para ir a máxima velocidad)\n",
    "            # time.sleep(0.01) \n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"🛑 Video detenido por el usuario.\")\n",
    "    finally:\n",
    "        cap.release()\n",
    "        print(\"Fin de la simulación.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b199b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de robustez\n",
    "def robustness_test(model, base_dir, classes, target_size):\n",
    "    if not os.path.exists(base_dir):\n",
    "        print(f\"⚠️ Directorio {base_dir} no encontrado.\")\n",
    "        return\n",
    "\n",
    "    def add_gaussian_noise(image, sigma):\n",
    "        \"\"\"Añade ruido gaussiano a una imagen normalizada (0-1).\"\"\"\n",
    "        if sigma == 0: return image\n",
    "        noise = np.random.normal(0, sigma, image.shape)\n",
    "        noisy_image = image + noise\n",
    "        return np.clip(noisy_image, 0.0, 1.0) # Asegurar rango válido\n",
    "\n",
    "    def apply_blur(image, kernel_size):\n",
    "        \"\"\"Aplica desenfoque gaussiano.\"\"\"\n",
    "        if kernel_size <= 1: return image\n",
    "        # Convertir a formato compatible con OpenCV si es necesario, pero cv2 soporta floats\n",
    "        return cv2.GaussianBlur(image, (kernel_size, kernel_size), 0)\n",
    "\n",
    "    def predict_single(model, img_array, classes):\n",
    "        \"\"\"Realiza la predicción sobre una imagen procesada.\"\"\"\n",
    "        img_batch = np.expand_dims(img_array, axis=0)\n",
    "        preds = model.predict(img_batch, verbose=0)\n",
    "        idx = np.argmax(preds)\n",
    "        return classes[idx], np.max(preds) * 100\n",
    "\n",
    "    # Iterar por cada clase (carpeta)\n",
    "    for class_folder in sorted(os.listdir(base_dir)):\n",
    "        folder_path = os.path.join(base_dir, class_folder)\n",
    "        if not os.path.isdir(folder_path): continue\n",
    "        \n",
    "        true_label = class_folder\n",
    "        if true_label not in classes: continue\n",
    "\n",
    "        # Buscar la imagen en la carpeta\n",
    "        files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        if not files:\n",
    "            print(f\"⚠️ No hay imágenes en {true_label}\")\n",
    "            continue\n",
    "            \n",
    "        img_path = os.path.join(folder_path, files[0]) # Cogemos la primera/única imagen\n",
    "        \n",
    "        # Cargar y preprocesar imagen base\n",
    "        original_img = tf.keras.preprocessing.image.load_img(img_path, target_size=target_size)\n",
    "        original_array = tf.keras.preprocessing.image.img_to_array(original_img)\n",
    "        original_array /= 255.0 # Normalizar a 0-1\n",
    "        \n",
    "        print(f\"\\n--- Evaluando Robustez para: {true_label} ---\")\n",
    "        \n",
    "        # Preparamos los plots\n",
    "        fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "        fig.suptitle(f\"Prueba de Estrés: {true_label}\", fontsize=16, weight='bold')\n",
    "\n",
    "        # --- FILA 1: RUIDO GAUSSIANO ---\n",
    "        for i, sigma in enumerate(NOISE_LEVELS):\n",
    "            img_noisy = add_gaussian_noise(original_array, sigma)\n",
    "            pred_lbl, conf = predict_single(model, img_noisy, classes)\n",
    "            \n",
    "            ax = axes[0, i]\n",
    "            ax.imshow(img_noisy)\n",
    "            ax.axis('off')\n",
    "            \n",
    "            color = 'green' if pred_lbl == true_label else 'red'\n",
    "            title_text = f\"Ruido: {sigma}\\nPred: {pred_lbl}\\nConf: {conf:.1f}%\"\n",
    "            ax.set_title(title_text, color=color, fontsize=10)\n",
    "\n",
    "        axes[0, 0].set_ylabel(\"Ruido\", fontsize=12, rotation=0, labelpad=40, weight='bold')\n",
    "\n",
    "        # --- FILA 2: DESENFOQUE (BLUR) ---\n",
    "        for i, k_size in enumerate(BLUR_LEVELS):\n",
    "            img_blurred = apply_blur(original_array, k_size)\n",
    "            pred_lbl, conf = predict_single(model, img_blurred, classes)\n",
    "            \n",
    "            ax = axes[1, i]\n",
    "            ax.imshow(img_blurred)\n",
    "            ax.axis('off')\n",
    "            \n",
    "            color = 'green' if pred_lbl == true_label else 'red'\n",
    "            blur_label = \"Original\" if k_size == 1 else f\"Blur (k={k_size})\"\n",
    "            title_text = f\"{blur_label}\\nPred: {pred_lbl}\\nConf: {conf:.1f}%\"\n",
    "            ax.set_title(title_text, color=color, fontsize=10)\n",
    "\n",
    "        axes[1, 0].set_ylabel(\"Blur\", fontsize=12, rotation=0, labelpad=40, weight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42ed4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejecutar_visualizacion_capas(model, img_dir, target_size, num_images=3):\n",
    "    \"\"\"\n",
    "    Función maestra que selecciona imágenes aleatorias de un directorio dado\n",
    "    y visualiza qué detectan las primeras y últimas capas convolucionales del modelo.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Función interna auxiliar para procesar una sola imagen ---\n",
    "    def _visualizar_una_imagen(modelo_base, ruta_imagen):\n",
    "        try:\n",
    "            # 1. Cargar imagen\n",
    "            img = tf.keras.preprocessing.image.load_img(ruta_imagen, target_size=target_size)\n",
    "            x = tf.keras.preprocessing.image.img_to_array(img)\n",
    "            x = x / 255.0\n",
    "            x = np.expand_dims(x, axis=0)\n",
    "\n",
    "            # 2. Detectar capas convolucionales\n",
    "            # Si el modelo usa Transfer Learning (ej. MobileNet), las capas están anidadas\n",
    "            motor_modelo = modelo_base\n",
    "            if isinstance(modelo_base.layers[0], tf.keras.Model):\n",
    "                motor_modelo = modelo_base.layers[0]\n",
    "\n",
    "            conv_layers = [layer for layer in motor_modelo.layers if 'conv' in layer.name]\n",
    "            \n",
    "            if not conv_layers:\n",
    "                print(\"⚠️ No se encontraron capas convolucionales.\")\n",
    "                return\n",
    "\n",
    "            # Seleccionamos la PRIMERA (texturas/bordes) y la ÚLTIMA (semántica/formas)\n",
    "            capas_seleccionadas = [conv_layers[0], conv_layers[-1]]\n",
    "            nombres_capas = [layer.name for layer in capas_seleccionadas]\n",
    "            \n",
    "            # Crear un mini-modelo que devuelva solo las salidas de estas capas\n",
    "            extractor = tf.keras.models.Model(inputs=motor_modelo.inputs, \n",
    "                                            outputs=[layer.output for layer in capas_seleccionadas])\n",
    "\n",
    "            # 3. Obtener activaciones\n",
    "            activaciones = extractor.predict(x, verbose=0)\n",
    "            \n",
    "            # 4. Pintar resultados\n",
    "            print(f\"\\n🔹 Analizando: {os.path.basename(ruta_imagen)}\")\n",
    "            \n",
    "            # Mostrar original pequeña\n",
    "            plt.figure(figsize=(3, 3))\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "            plt.title(\"Entrada Original\", fontsize=10)\n",
    "            plt.show()\n",
    "\n",
    "            titulos = [\"PRIMERAS CAPAS (Bordes/Texturas)\", \"ÚLTIMAS CAPAS (Semántica/Abstracto)\"]\n",
    "            \n",
    "            for nombre_capa, activacion, titulo in zip(nombres_capas, activaciones, titulos):\n",
    "                # La activación tiene forma (1, alto, ancho, canales)\n",
    "                n_filtros = min(16, activacion.shape[-1]) # Máximo 16 filtros para no saturar\n",
    "                size = activacion.shape[1] \n",
    "                \n",
    "                n_cols = 4\n",
    "                n_rows = (n_filtros + n_cols - 1) // n_cols\n",
    "                \n",
    "                grid_img = np.zeros((size * n_rows, size * n_cols))\n",
    "\n",
    "                for i in range(n_rows):\n",
    "                    for j in range(n_cols):\n",
    "                        idx_filtro = i * n_cols + j\n",
    "                        if idx_filtro < n_filtros:\n",
    "                            filtro_img = activacion[0, :, :, idx_filtro]\n",
    "                            \n",
    "                            # Normalización visual para mejorar contraste\n",
    "                            if filtro_img.std() != 0:\n",
    "                                filtro_img -= filtro_img.mean()\n",
    "                                filtro_img /= filtro_img.std()\n",
    "                                filtro_img *= 64\n",
    "                                filtro_img += 128\n",
    "                            \n",
    "                            filtro_img = np.clip(filtro_img, 0, 255).astype('uint8')\n",
    "                            grid_img[i*size : (i+1)*size, j*size : (j+1)*size] = filtro_img\n",
    "\n",
    "                scale = 1.5\n",
    "                plt.figure(figsize=(scale * n_cols, scale * n_rows))\n",
    "                plt.title(f\"{titulo}\\nCapa: {nombre_capa}\", fontsize=12, weight='bold')\n",
    "                plt.imshow(grid_img, aspect='auto', cmap='viridis') \n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error visualizando {os.path.basename(ruta_imagen)}: {e}\")\n",
    "\n",
    "    # --- Lógica Principal ---\n",
    "    if not os.path.exists(img_dir):\n",
    "        print(f\"⚠️ El directorio {img_dir} no existe.\")\n",
    "        return\n",
    "\n",
    "    # Buscar imágenes (solo nivel plano)\n",
    "    archivos = [os.path.join(img_dir, f) for f in os.listdir(img_dir) \n",
    "                if f.lower().endswith(('.png', '.jpg', '.jpeg', '.webp'))]\n",
    "\n",
    "    if archivos:\n",
    "        seleccion = random.sample(archivos, min(num_images, len(archivos)))\n",
    "        print(f\"--- Iniciando Visualización de Capas ({len(seleccion)} imágenes) ---\")\n",
    "        for ruta in seleccion:\n",
    "            _visualizar_una_imagen(model, ruta)\n",
    "            print(\"-\" * 60)\n",
    "    else:\n",
    "        print(f\"⚠️ No hay imágenes en {img_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fd21db",
   "metadata": {},
   "source": [
    "### Ejecución de los experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2594b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_wild_predictions(final_model, TEST_IMAGES_DIR, class_names, TARGET_SIZE)\n",
    "process_and_display_video(VIDEO_PATH, final_model, class_names, TARGET_SIZE)\n",
    "robustness_test(final_model, TEST_SINGLE_IMG_DIR, class_names, TARGET_SIZE)\n",
    "ejecutar_visualizacion_capas(final_model, TEST2_IMAGES_DIR, TARGET_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
